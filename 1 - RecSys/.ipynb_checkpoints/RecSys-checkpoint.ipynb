{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018 Edition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a music recommender system\n",
    "\n",
    "As its name implies, a recommender system is a tool that helps predicting what a user may or may not like among a list of given items. In some sense, you can view this as an alternative to content search, as recommendation engines help users discover products or content that they may not come across otherwise. For example, Facebook suggests friends and pages to users. Youtube recommends videos which users may be interested in. Amazon suggests the products which users may need... Recommendation engines engage users to services, can be seen as a revenue optimization process, and in general help maintaining interest in a service.\n",
    "\n",
    "In this notebook, we study how to build a simple recommender system: we focus on music recommendations, and we use a simple algorithm to predict which items users might like, that is called ALS, alternating least squares.\n",
    "\n",
    "## Goals\n",
    "\n",
    "In this lecture, we expect students to:\n",
    "\n",
    "- Revisit (or learn) recommender algorithms\n",
    "\n",
    "- Understand the idea of Matrix Factorization and the ALS algorithm (serial and parallel versions)\n",
    "\n",
    "- Build a simple model for a real usecase: music recommender system\n",
    "\n",
    "- Understand how to validate the results\n",
    "\n",
    "## Steps\n",
    "\n",
    "We assume students to work outside lab hours on the learning material. These are the steps by which we guide students, during labs, to build a good basis for the end-to-end development of a recommender system:\n",
    "\n",
    "* Inspect the data using Spark SQL, and build some basic, but very valuable knowledge about the information we have at hand\n",
    "* Formally define what is a sensible algorithm to achieve our goal: given the \"history\" of user taste for music, recommend new music to discover. Essentialy, we want to build a statistical model of user preferences such that we can use it to \"predict\" which additional music the user could like\n",
    "* With our formal definition at hand, we will learn different ways to implement such an algorithm. Our goal here is to illustrate what are the difficulties to overcome when implementing a (parallel) algorithm\n",
    "* Finally, we will focus on an existing implementation, available in the Apache Spark MLLib, which we will use out of the box to build a reliable statistical model\n",
    "\n",
    "Now, you may think at this point we will be done!\n",
    "\n",
    "Well, you'd better think twice: one important topic we will cover in all our Notebooks is **how to validate the results we obtain**, and **how to choose good parameters to train models** especially when using an \"opaque\" library for doing the job. As a consequence, we will focus on the statistical validation of our recommender system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data\n",
    "\n",
    "Understanding data is one of the most important part when designing any machine learning algorithm. In this notebook, we will use a data set published by Audioscrobbler - a music recommendation system for last.fm. Audioscrobbler is also one of the first internet streaming radio sites, founded in 2002. It provided an open API for “scrobbling”, or recording listeners’ plays of artists’ songs. last.fm used this information to build a powerful music recommender engine.\n",
    "\n",
    "## 1.1. Data schema\n",
    "\n",
    "Unlike a rating dataset which contains information about users' preference for products (one star, 3 stars, and so on), the datasets from Audioscrobbler only has information about events: specifically, it keeps track of how many times a user played songs of a given artist and the names of artists. That means it carries less information than a rating: in the literature, this is called explicit vs. implicit ratings.\n",
    "\n",
    "### Reading material\n",
    "\n",
    "- [Implicit Feedback for Inferring User Preference: A Bibliography](http://people.csail.mit.edu/teevan/work/publications/papers/sigir-forum03.pdf)\n",
    "- [Comparing explicit and implicit feedback techniques for web retrieval: TREC-10 interactive track report](http://trec.nist.gov/pubs/trec10/papers/glasgow.pdf)\n",
    "- [Probabilistic Models for Data Combination in Recommender Systems](http://mlg.eng.cam.ac.uk/pub/pdf/WilGha08.pdf)\n",
    "\n",
    "The data we use in this Notebook is available in 3 files (these files are stored in our HDFS layer, in the directory  ```/datasets/lastfm```):\n",
    "\n",
    "- **`user_artist_data.txt`**: It contains about 140,000+ unique users, and 1.6 million unique artists. About 24.2 million users’ plays of artists’ are recorded, along with their count. It has 3 columns separated by spaces: \n",
    "\n",
    "| UserID | ArtistID | PlayCount |\n",
    "|----|----|----|\n",
    "| ...|...|...|\n",
    "\n",
    "\n",
    "- **`artist_data.txt`** : It prodives the names of each artist by their IDs. It has 2 columns separated by tab characters (`\\t`).\n",
    "\n",
    "| ArtistID | Name |\n",
    "|---|---|\n",
    "|...|...|\n",
    "\n",
    "- **`artist_alias.txt`**: Note that when plays are scrobbled, the client application submits the name of the artist being played. This name could be misspelled or nonstandard. For example, \"The Smiths\", \"Smiths, The\", and \"the smiths\" may appear as distinct artist IDs in the data set, even though they are plainly the same. `artist_alias.txt` maps artist IDs that are known misspellings or variants to the canonical ID of that artist. The data in this file has 2 columns separated by tab characters (`\\t`).\n",
    "\n",
    "| MisspelledArtistID | StandardArtistID |\n",
    "|---|---|\n",
    "|...|...|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Understanding data: simple descriptive statistic\n",
    "\n",
    "In order to choose or design a suitable algorithm for achieving our goals, given the data we have, we should first understand data characteristics. To start, we import the necessary packages to work with regular expressions, Data Frames, and other nice features of our programming environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(palette=sns.color_palette(\"colorblind\", 8),\n",
    "       style=sns.axes_style(\"white\"))\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "base = \"/datasets/lastfm/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "#### Question 1.0 (Non-grading)\n",
    "<div class=\"alert alert-info\">\n",
    "Using SPARK SQL, load data from `/datasets/lastfm/user_artist_data.txt` and show the first 20 entries (via function `show()`).\n",
    "</div>\n",
    "\n",
    "For this Notebook, from a programming point of view, we are given the schema for the data we use, which is as follows:\n",
    "\n",
    "```\n",
    "userID: long int\n",
    "artistID: long int\n",
    "playCount: int\n",
    "```\n",
    "\n",
    "Each line of the dataset contains the above three fields, separated by a \"white space\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+\n",
      "| userID|artistID|playCount|\n",
      "+-------+--------+---------+\n",
      "|1000002|       1|       55|\n",
      "|1000002| 1000006|       33|\n",
      "|1000002| 1000007|        8|\n",
      "|1000002| 1000009|      144|\n",
      "|1000002| 1000010|      314|\n",
      "|1000002| 1000013|        8|\n",
      "|1000002| 1000014|       42|\n",
      "|1000002| 1000017|       69|\n",
      "|1000002| 1000024|      329|\n",
      "|1000002| 1000025|        1|\n",
      "|1000002| 1000028|       17|\n",
      "|1000002| 1000031|       47|\n",
      "|1000002| 1000033|       15|\n",
      "|1000002| 1000042|        1|\n",
      "|1000002| 1000045|        1|\n",
      "|1000002| 1000054|        2|\n",
      "|1000002| 1000055|       25|\n",
      "|1000002| 1000056|        4|\n",
      "|1000002| 1000059|        2|\n",
      "|1000002| 1000062|       71|\n",
      "+-------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userArtistDataSchema = StructType([ \\\n",
    "    StructField(\"userID\", LongType(), True), \\\n",
    "    StructField(\"artistID\", LongType(), True), \\\n",
    "    StructField(\"playCount\", IntegerType(), True)])\n",
    "\n",
    "userArtistDF = sqlContext.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='false', delimiter=' ') \\\n",
    "    .load(base + \"user_artist_data.txt\", schema = userArtistDataSchema) \\\n",
    "    .cache()\n",
    "\n",
    "# we can cache an Dataframe to avoid computing it from the beginning everytime it is accessed.\n",
    "userArtistDF.cache()\n",
    "\n",
    "userArtistDF.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.1: \n",
    "<div class=\"alert alert-info\">\n",
    "How many distinct users do we have in our data? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total n. of users:  148111\n"
     ]
    }
   ],
   "source": [
    "uniqueUsers = userArtistDF.select(\"UserID\").distinct().count()\n",
    "print(\"Total n. of users: \", uniqueUsers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    There are approximately 150k distinct users, which is enough to use a statistical approach as we will do later in this notebook.\n",
    "</p>\n",
    "<p>\n",
    "    A too few number of users would not have been sufficient to predict accurately users tastes as all the sensitivities would not have been represented in the dataset.\n",
    "</p>\n",
    "<p>\n",
    "    Furthermore, a huge number of distinct and various users reduces the risk of overfitting if those users have a huge variety of tastes. In fact this widens the range of features that can be predicted accurately using a <b>Collaborative-based system</b> (this will be the case in this notebook because <b>content-based systems</b> needs more features as there is in the dataset for artists (for instance, it would have been accurate to include the genre, the year, or even the main language of the artist).\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional question\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "What is the average number of entries per user ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average user entries:  164\n"
     ]
    }
   ],
   "source": [
    "entriesPerUser = userArtistDF.select(\"UserID\").count()//\\\n",
    "    userArtistDF.select(\"UserID\").distinct().count()#Floored\n",
    "print(\"Average user entries: \", entriesPerUser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    The more entries there will be per user, the less sparse (<i>warmer</i>) the utility matrix will be and therefore the more the recommandation will be accurate (it also improves the risk of overfitting). \n",
    "</p>\n",
    "<p>\n",
    "This is what happend when we gather people in groups of interest to improve the prediction on preferences : this is reducing the number of artists to take into account (is nobody in the group ever heard/rated an artist, there is no need to include it in the matrix and it therefore reduces the number of it). The goal is obviously to have a more dense matrix, easier to exploit.\n",
    "</p>\n",
    "<p>\n",
    "Generaly speaking, on a huge dataset, the number of feature per observation is relatively slow, leading to a pretty sparse matrix and a huge degree of liberty predicting good results: there is a lot of uncertainty.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Another important thing to take into account is (cf question 3.2) that artists may be mispelled or non-standard and therefore the matrix may be way less sparse. This is the reason why we don't compare the number of entries per user to the number of artists for now.\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2\n",
    "<div class=\"alert alert-info\">\n",
    "How many distinct artists do we have in our data ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total n. of artists:  1631028\n"
     ]
    }
   ],
   "source": [
    "uniqueArtists = userArtistDF.select(\"ArtistID\").distinct().count()\n",
    "print(\"Total n. of artists: \", uniqueArtists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    In the opposite of the number of Users, the number of artists increases the sparsity of the utility matrix in the case of collaborative-based system and therefore makes predictions less accurate. A system is capable of better prediction in a smaller set of products.\n",
    "</p>\n",
    "<p>\n",
    "    But here, as we will see later in this notebook, the number of distinct artists is not right: artists may have been mispelled in that dataset.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3\n",
    "<div class=\"alert alert-info\">\n",
    "One limitation of Spark MLlib's ALS implementation - which we will use later - is that it requires IDs for users and items to be nonnegative 32-bit integers. This means that IDs larger than `Integer.MAX_VALUE`, or `2147483647`, can't be used. So we need to check whether this data set conforms to the strict requirements of our library.  \n",
    "\n",
    "What are the maximum and minimum values of column `userID` ?  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum user ID is  2443548  <=  Integer.MAX_VALUE\n"
     ]
    }
   ],
   "source": [
    "maxUserID = (userArtistDF.select(max(\"UserID\"))\n",
    "             # The returned tab is of len 1\n",
    "             .collect()[0]\n",
    "             # Easier to process as a dictionnary\n",
    "             .asDict()[\"max(UserID)\"])\n",
    "\n",
    "print(\"The maximum user ID is \", \n",
    "      maxUserID, \n",
    "      \" <= \" if maxUserID <= 2147483647 else \" > \",\n",
    "      \"Integer.MAX_VALUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum user ID is  90  >=  0\n"
     ]
    }
   ],
   "source": [
    "minUserID = (userArtistDF.select(min(\"UserID\"))\n",
    "             # The returned tab is of len 1\n",
    "             .collect()[0]\n",
    "             # Easier to process as a dictionnary\n",
    "             .asDict()[\"min(UserID)\"])\n",
    "\n",
    "print(\"The minimum user ID is \", \n",
    "      minUserID, \n",
    "      \" >= \" if minUserID >= 0 else \" < \",\n",
    "      \"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.4\n",
    "<div class=\"alert alert-info\">\n",
    "What is the maximum and minimum values of column `artistID` ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum artist ID is  10794401  <=  Integer.MAX_VALUE\n"
     ]
    }
   ],
   "source": [
    "maxArtistID = (userArtistDF.select(max(\"ArtistID\"))\n",
    "               .collect()[0]\n",
    "               .asDict()[\"max(ArtistID)\"])\n",
    "\n",
    "print(\"The maximum artist ID is \", \n",
    "      maxArtistID, \n",
    "      \" <= \" if maxArtistID <= 2147483647 else \" > \",\n",
    "      \"Integer.MAX_VALUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum artist ID is  1  >=  0\n"
     ]
    }
   ],
   "source": [
    "minArtistID = (userArtistDF.select(min(\"ArtistID\"))\n",
    "               .collect()[0]\n",
    "               .asDict()[\"min(ArtistID)\"])\n",
    "\n",
    "print(\"The minimum artist ID is \", \n",
    "      minArtistID, \n",
    "      \" >= \" if minArtistID >= 0 else \" < \",\n",
    "      \"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just discovered that we have a total of 148,111 users in our dataset. Similarly, we have a total of 1,631,028 artists in our dataset. The maximum values of `userID` and `artistID` are still smaller than the biggest number of integer type.  No additional transformation will be necessary to use these IDs.\n",
    "\n",
    "One thing we can see here is that SPARK SQL provides very concise and powerful methods for data analytics (compared to using RDD and their low-level API). You can see more examples [here](https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we might want to understand better user activity and artist popularity.\n",
    "\n",
    "Here is a list of simple descriptive queries that helps us reaching these purposes:\n",
    "\n",
    "* How many times each user has played a song? This is a good indicator of who are the most active users of our service. Note that a very active user with many play counts does not necessarily mean that the user is also \"curious\"! Indeed, she could have played the same song several times.\n",
    "* How many play counts for each artist? This is a good indicator of the artist popularity. Since we do not have time information associated to our data, we can only build a, e.g., top-10 ranking of the most popular artists in the dataset. Later in the notebook, we will learn that our dataset has a very \"loose\" definition about artists: very often artist IDs point to song titles as well. This means we have to be careful when establishing popular artists. Indeed, artists whose data is \"well formed\" will have the correct number of play counts associated to them. Instead, artists that appear mixed with song titles may see their play counts \"diluted\" across their songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "#### Question 2.1\n",
    "<div class=\"alert alert-info\">\n",
    "How many times each user has played a song? Show 5 samples of the result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum(PlayCount)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2289066</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289173</th>\n",
       "      <td>5938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290086</th>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291343</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291463</th>\n",
       "      <td>8802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sum(PlayCount)\n",
       "UserID                 \n",
       "2289066              51\n",
       "2289173            5938\n",
       "2290086            1080\n",
       "2291343              37\n",
       "2291463            8802"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute user activity\n",
    "# We are interested in how many playcounts each user has scored.\n",
    "userActivity = userArtistDF.groupBy(\"UserID\").sum(\"PlayCount\").collect()\n",
    "# Plot the number of Play per user (for 5 users)\n",
    "# Pandas allow us to do that in a visual way\n",
    "pd.DataFrame(userActivity[:5], \n",
    "             columns=userActivity[0].__fields__)\\\n",
    "  .set_index(\"UserID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|min(sum(PlayCount))|max(sum(PlayCount))|\n",
      "+-------------------+-------------------+\n",
      "|                  1|             674412|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at the min and the max activity for users\n",
    "(userArtistDF\n",
    "        # We want to sum the PlayCount field for each user\n",
    "        .groupBy(\"UserID\")\n",
    "        .sum(\"PlayCount\")\n",
    "        # Then process the max and the min\n",
    "        .select(min(\"sum(PlayCount)\"), max(\"sum(PlayCount)\"))\n",
    "        .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    User activity is very variable: it starts from new users or users that only listen to a few songs to users with hundreds of thousand of listening.\n",
    "</p>\n",
    "<p>\n",
    "    This has to do with the time spent on the LastFM platform. Multiple behaviours can be observed:\n",
    "    <ul>\n",
    "        <li>Some users are very curious and spend a lot of time looking for new songs and to widen the range of their listenings. Those users are very useful for the recommendation system as they realy improve the accuracy of the prediction, by making the utility matrix warmer (less sparse) ; their rating (here number of listening) is then very usefull. However, the drawback is that a recommendation system leads to reduce the variety of their listening by predicting what they want to listen and therefore to reduce their curiosity. This can leads to entire parts of the matrix that are unexplored by users if they only follow the recommendation system.</li>\n",
    "        <li>Some users juste loop-play the music they already know. Those users doesn't contribute much to improve the utility matrix, but may be more likely to benefit from the recommendation system. However they are still usefull for other users that have the same tastes as them and also not listening so many artists but knowing a little bit less songs.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2\n",
    "<div class=\"alert alert-info\">\n",
    "Plot CDF (or ECDF) of the number of play counts per User ID.  \n",
    "\n",
    "Explain and comment the figure you just created:   \n",
    "<ul>\n",
    "<li>for example, look at important percentiles (25%, median, 75%, tails such as >90%) and cross check with what you have found above to figure out if the result is plausible. </li>\n",
    "<li>discuss about your users, with respect to the application domain we target in the notebook: you will notice that for some users, there is very little interaction with the system, which means that maybe reccommending something to them is going to be more difficult than for other users who interact more with the system. </li>\n",
    "<li>look at outliers and reason about their impact on your reccommender algorithm</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = [.25,.5,.75,.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      1      2 ... 148108 148109 148110]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c3819bf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdf = pd.DataFrame(data=userActivity)\n",
    "Y=np.sort( pdf[1] )\n",
    "data_nb = len(Y)\n",
    "yvals=np.arange(len(Y))/float(data_nb)\n",
    "\n",
    "print(np.arange(len(Y)))\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.plot( Y, yvals )\n",
    "plt.xlabel('Play Counts')\n",
    "plt.ylabel('ECDF')\n",
    "plt.grid(True,which=\"both\",ls=\"-\")\n",
    "for percentile in percentiles:\n",
    "    plt.axhline(percentile, color='r', linestyle=\"--\")\n",
    "    plt.axvline(Y[int(data_nb*percentile)], color='r', linestyle='--')\n",
    "plt.title('ECDF of number of play counts per User ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentiles</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>2800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>6484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             value\n",
       "percentiles       \n",
       "0.25           204\n",
       "0.50           892\n",
       "0.75          2800\n",
       "0.90          6484"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile_values = list(zip(percentiles, \n",
    "                             [Y[int(data_nb*p)] for p in percentiles]))\n",
    "pd.DataFrame(percentile_values, \n",
    "             columns=[\"percentiles\", \"value\"])\\\n",
    "  .set_index(\"percentiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to eliminate outliers (See question 11.2)\n",
    "user_range = dict(min=Y[int(data_nb*.1)], \n",
    "                  max=Y[int(data_nb*.9)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "The huge majority of users have a limited number of play each (typically, 90% of the users have an individual play count of less than 6500).\n",
    "</p>\n",
    "<p>\n",
    "    As said in the previous question, some users interacts a lot with the systems. Those users will benefit and contribute a lot to the recommendation system. The users that don't contribute much to the system, listening to music, or the new users will have biased recommendations or too generic recommendations:<br>\n",
    "    An user that have no contribution will have generic prediction based on the whole system as for the \"most viewed\" homepage in YouTube.<br>\n",
    "    Some users that contributed too few will have their recommendation based only on their contribution, which can be on the boundaries of what they like, or not focused on the typical listening they want. In that case the recommendation system which is collaborative and therefore based on other users feedback will not associate this user to the right group of users which would give good answers.\n",
    "</p>\n",
    "<br>\n",
    "<p>\n",
    "    <b>Example:</b> A user with 3 listening, 1 on something he doesn't like (e.g hard-rock) and listened for some random reason, and 2 things he likes (e.g generic, well known classical music) will have biased recommended content as hard-rock remix of classical music, if the users he is associated are listening this knid of music. <br>\n",
    "    \n",
    "    This also stresses the importance to have a huge user and listening base to start such a recommendation system.\n",
    "</p>\n",
    "<br>\n",
    "<p>\n",
    "    Finaly users that listen too much to music are not that interesting because they are likely to let their PC turned on with the music played, even without choosing it, changing automatically with the recommandations given.\n",
    "</p>\n",
    "<br>\n",
    "<p class=\"alert alert-warning\">\n",
    "    All of these reasons lead to eliminate the \"outliers\" that are here present and only keep the range [10%-90%]. As a matter of fact we belive that this satifies the central-limit theorem which tells us that a huge number of users will have a gaussian behaviour.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.3\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "How many play counts for each artist? Plot CDF or ECDF of the result.  \n",
    "\n",
    "Similarly to the previous question, you need to comment and interpret your result: what is the figure telling you?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum(PlayCount)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ArtistID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1003514</th>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004346</th>\n",
       "      <td>3772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5409</th>\n",
       "      <td>526693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002519</th>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004223</th>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sum(PlayCount)\n",
       "ArtistID                \n",
       "1003514              949\n",
       "1004346             3772\n",
       "5409              526693\n",
       "1002519              405\n",
       "1004223              409"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute artist popularity\n",
    "# We are interested in how many playcounts per artist\n",
    "# ATTENTION! Grouping by artistID may be problematic, as stated above.\n",
    "\n",
    "artistPopularity = userArtistDF.groupBy(\"ArtistID\").sum(\"PlayCount\").collect()\n",
    "# Plot the number of Play per artist (for 5 users)\n",
    "# Pandas allow us to do that in a visual way\n",
    "pd.DataFrame(artistPopularity[:5], \n",
    "             columns=artistPopularity[0].__fields__)\\\n",
    "  .set_index(\"ArtistID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p>The PlayCount per artist is a good indicator of the artist popularity. The more the artist is listened the more he is popular.</p>\n",
    "    <p>The importance of knowing popularity comes from the importance of ratings as it is importance ton know the preferences of a user for collaborative filtering. That's what will be used for gathering 'similar' users according to preferences.</p>\n",
    "    <p>But there is a problem : very often artist IDs point to song titles as well. This means we have to be careful when establishing popular artists. Indeed, artists whose data is \"well formed\" will have the correct number of play counts associated to them. Instead, artists that appear mixed with song titles may see their play counts shared across their songs.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0       1       2 ... 1631025 1631026 1631027]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c2d7f40b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdf = pd.DataFrame(data=artistPopularity)\n",
    "Y=np.sort( pdf[1] )\n",
    "data_nb = len(Y)\n",
    "yvals=np.arange(data_nb)/float(data_nb)\n",
    "\n",
    "print(np.arange(data_nb))\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.plot( Y, yvals )\n",
    "plt.xlabel('Play Counts')\n",
    "plt.ylabel('ECDF')\n",
    "plt.grid(True,which=\"both\",ls=\"-\")\n",
    "for percentile in percentiles:\n",
    "    plt.axhline(percentile, color='r', linestyle=\"--\")\n",
    "    plt.axvline(Y[int(data_nb*percentile)], color='r', linestyle='--')\n",
    "plt.title('ECDF of number of play counts per Artist ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentiles</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             value\n",
       "percentiles       \n",
       "0.25             1\n",
       "0.50             3\n",
       "0.75            11\n",
       "0.90            45"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile_values = list(zip(percentiles, \n",
    "                             [Y[int(data_nb*p)] \n",
    "                              for p in percentiles]))\n",
    "\n",
    "pd.DataFrame(percentile_values, \n",
    "             columns=[\"percentiles\", \"value\"])\\\n",
    "  .set_index(\"percentiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to eliminate outliers (See question 11.2)\n",
    "\n",
    "# The min is set to 50% here as those are \n",
    "# artists with less than 4 playcount\n",
    "# Note than there is here no max: \n",
    "# it would be irrelevant to withdraw \n",
    "# artists that listened a lot.\n",
    "\n",
    "artist_range = dict(min=Y[int(data_nb*.4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "Same as for the user listenings, only a few artists makes almost all listenings. This can be interpretated with at least two reasons :\n",
    "    <ul>\n",
    "        <li>A certain number of artists are mispelled and therefore have a few listening count on the wrong name.</li>\n",
    "        <li>The artists supported by major labels are most likely to be listened a huge number of times, whereas the number of listening of less known artists is very small. This phenomenon is know as the \"long tail\" phenomenon: with an amount of artist proposed really big, users tends to listen the artists well known and already listened by a lot of users.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>25% of artists are listened less than 1 time, and the huge majority of them (90%) are only listened less than 45 times, which is extremely small. Most of the artists are not well known, which explains this fact.(Again the long trail phenomenon) </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.4\n",
    "<div class=\"alert alert-info\">\n",
    "Plot a bar chart to show top 5 artists In terms of absolute play counts.  \n",
    "\n",
    "Comment the figure you just obtained: \n",
    "<ul>\n",
    "<li>are these reasonable results?</li>\n",
    "<li>is looking at top-5 artists enough to learn more about your data?</li>\n",
    "<li>do you see anything strange in the data?</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c1649b668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sortedArtist = sorted(artistPopularity, key = lambda x: -x[\"sum(PlayCount)\"])[:5]\n",
    "\n",
    "artistID = [w[\"ArtistID\"] for w in sortedArtist]\n",
    "\n",
    "y_pos = range(len(sortedArtist))\n",
    "frequency = [w[1] for w in sortedArtist]\n",
    "\n",
    "plt.barh(y_pos, frequency[::-1], align='center', alpha=0.4)\n",
    "plt.yticks(y_pos, artistID[::-1])\n",
    "plt.xlabel('Play Count')\n",
    "plt.ylabel('Artist')\n",
    "plt.title('Top-5 Artist ID per play counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<li>The results seem to be reasonable: the play count number can be very big and those numbers and the one we saw previously for the artists less listened corresponds to the long trail phenomenon. </li>\n",
    "<li>However, looking only at the top 5 is not enough for many reasons. For example, some users may be listening to the same artist many times and this graph won't enable us to figure this out.</li>\n",
    "<li> And the weird thing here is that only in the top 5 artists, there is a big variation between the first and the fifth (2500000 vs 1400000). We can't know if it's because of the artist IDs that are pointing to songs which dilutes the play count for the fifth artist for example.</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additionnal question\n",
    "<div class=\"alert alert-info\">\n",
    "What is the percentage of views made by the top 1% of artists ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 1% artists makes 89.23% of all the listenings.\n"
     ]
    }
   ],
   "source": [
    "top1p = Y[int(data_nb*.99)]\n",
    "\n",
    "print(\"The top 1% artists makes {:.2f}% of all the listenings.\"\n",
    "      .format(np.sum(Y[Y>top1p])*100/np.sum(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    This verifies the previous remarks on the <i>long-tail</i> phenomenon.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All seems clear right now, but ... wait a second! What about the problems indicated above about artist \"disambiguation\"? Are these artist ID we are using referring to unique artists? How can we make sure that such \"opaque\" identifiers point to different bands? Let's try to use some additional dataset to answer this question:  `artist_data.txt` dataset. This time, the schema of the dataset consists in:\n",
    "\n",
    "```\n",
    "artist ID: long int\n",
    "name: string\n",
    "```\n",
    "\n",
    "We will try to find whether a single artist has two different IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "#### Question 3.1\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Load the data from `/datasets/lastfm/artist_data.txt` and use the SparkSQL API to show 5 samples.  \n",
    "\n",
    "<ul></ul>\n",
    "<div class=\"label label-success\">HINT:</div> If you encounter some error when parsing lines in data because of invalid entries, parameter `mode='DROPMALFORMED'` will help you to eliminate these entries. The suggested syntax is: `<df>.options(header='false', delimiter='\\t', mode='DROPMALFORMED')`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|artistID|                name|\n",
      "+--------+--------------------+\n",
      "| 1134999|        06Crazy Life|\n",
      "| 6821360|        Pang Nakarin|\n",
      "|10113088|Terfel, Bartoli- ...|\n",
      "|10151459| The Flaming Sidebur|\n",
      "| 6826647|   Bodenstandig 3000|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customSchemaArtist = StructType([ \\\n",
    "    StructField(\"artistID\", LongType(), True), \\\n",
    "    StructField(\"name\", StringType(), True)])\n",
    "\n",
    "artistDF = (\n",
    "            sqlContext.read \n",
    "                .format('com.databricks.spark.csv') \n",
    "                # Drop the invalid entries\n",
    "                .options(header='false', delimiter='\\t', mode='DROPMALFORMED')\n",
    "                # The schema is the one above\n",
    "                .load(base + \"artist_data.txt\", schema = customSchemaArtist)\n",
    "            )\n",
    "\n",
    "# we can cache an Dataframe to avoid computing it from the beginning everytime it is accessed.\n",
    "artistDF.cache()\n",
    "\n",
    "# Ensure the data are correctly loaded\n",
    "artistDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.2\n",
    "<div class=\"alert alert-info\">\n",
    "Find 20 artists whose name contains `Aerosmith`. Take a look at artists that have ID equal to `1000010` and `2082323`. In your opinion, are they pointing to the same artist?  \n",
    "\n",
    "<ul></ul>\n",
    "<div class=\"label label-success\">HINT:</div> Function `locate(sub_string, string)` can be useful in this case.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|artistID|                name|\n",
      "+--------+--------------------+\n",
      "|10586006|Dusty Springfield...|\n",
      "| 6946007|    Aerosmith/RunDMC|\n",
      "|10475683|Aerosmith: Just P...|\n",
      "| 1083031|    Aerosmith/ G n R|\n",
      "| 6872848|Britney, Nsync, N...|\n",
      "|10586963|Green Day - Oasis...|\n",
      "|10028830|The Aerosmith Ant...|\n",
      "|10300357| Run-DMC + Aerosmith|\n",
      "| 2027746|Aerosmith by Musi...|\n",
      "| 1140418|[rap]Run DMC and ...|\n",
      "|10237208| Aerosmith + Run DMC|\n",
      "|10588537|Aerosmith, Kid Ro...|\n",
      "| 9934757|Aerosmith - Big Ones|\n",
      "|10437510|Green Day ft. Oas...|\n",
      "| 6936680| RUN DNC & Aerosmith|\n",
      "|10479781|      Aerosmith Hits|\n",
      "|10114147|Charlies Angels -...|\n",
      "| 1262439|Kid Rock, Run DMC...|\n",
      "| 7032554|Aerosmith & Run-D...|\n",
      "|10033592|          Aerosmith?|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---------+\n",
      "|artistID|     name|\n",
      "+--------+---------+\n",
      "| 1000010|Aerosmith|\n",
      "+--------+---------+\n",
      "\n",
      "+--------+------------+\n",
      "|artistID|        name|\n",
      "+--------+------------+\n",
      "| 2082323|01 Aerosmith|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get artists whose name contains \"Aerosmith\"\n",
    "artistDF[locate(\"Aerosmith\", \"name\") > 0].show(20)\n",
    "\n",
    "# show two examples\n",
    "artistDF[artistDF.artistID==1000010].show()\n",
    "artistDF[artistDF.artistID==2082323].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    Many artists names contains <cite>Aerosmith</cite>, among them, there are duplicates for this music group (and surely for other artists). We can see that the name of the artist is accompanied with a song name and others or just mispelled. We then have to remove duplicates by adressing them to the right artist.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question correctly, we need to use an additional dataset `artist_alias.txt` which contains the ids of mispelled artists and standard artists. The schema of the dataset consists in:\n",
    "\n",
    "```\n",
    "mispelledID ID: long int\n",
    "standard ID: long int\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.3\n",
    "<div class=\"alert alert-info\">\n",
    "Using SparkSQL API, load the dataset from `/datasets/lastfm/artist_alias.txt` then show 5 samples.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|MispelledArtistID|StandardArtistID|\n",
      "+-----------------+----------------+\n",
      "|          1092764|         1000311|\n",
      "|          1095122|         1000557|\n",
      "|          6708070|         1007267|\n",
      "|         10088054|         1042317|\n",
      "|          1195917|         1042317|\n",
      "+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customSchemaArtistAlias = StructType([ \\\n",
    "    StructField( \"MispelledArtistID\", LongType(), True ), \\\n",
    "    StructField( \"StandardArtistID\", LongType(), True )])\n",
    "\n",
    "# We have to drop NA values, as is doesn't provide reliable information.\n",
    "\n",
    "artistAliasDF = (\n",
    "                sqlContext.read \\\n",
    "                    .format('com.databricks.spark.csv') \n",
    "                    .options(header='false', delimiter='\\t', mode='DROPMALFORMED') \n",
    "                    .load(base + \"artist_alias.txt\", schema = customSchemaArtistAlias)\n",
    "                    # Drop the invalid entries: some entries does not have both a key and a value\n",
    "                    .dropna() \n",
    "                )\n",
    "    \n",
    "# we can cache an Dataframe to avoid computing it from the beginning everytime it is accessed.\n",
    "artistAliasDF.cache()\n",
    "\n",
    "# Ensure the data are correctly loaded\n",
    "artistAliasDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.4\n",
    "<div class=\"alert alert-info\">\n",
    "Verify the answer of question 3.2 (\"Are artists that have ID equal to `1000010` and `2082323` the same ?\") by finding the standard ids corresponding to the mispelled ids `1000010` and `2082323` respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|MispelledArtistID|StandardArtistID|\n",
      "+-----------------+----------------+\n",
      "+-----------------+----------------+\n",
      "\n",
      "+-----------------+----------------+\n",
      "|MispelledArtistID|StandardArtistID|\n",
      "+-----------------+----------------+\n",
      "|          2082323|         1000010|\n",
      "+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistAliasDF[ artistAliasDF[\"MispelledArtistID\"] == 1000010 ].show()\n",
    "artistAliasDF[ artistAliasDF[\"MispelledArtistID\"] == 2082323 ].show()\n",
    "\n",
    "# 1000010 is a standard id, so it haven't been considered as mispelled id in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    Real artists don't have any entry in the aliases dataset.\n",
    "</p>\n",
    "    It confirms that '01 Aerosmith' and 'Aerosmith' correspond to the same artist.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "The mispelled or nonstandard information about artist make our results in the previous queries a bit \"sloppy\". To overcome this problem, we can replace all mispelled artist ids by the corresponding standard ids and re-compute the basic descriptive statistics on the \"amended\" data.\n",
    "First, we construct a \"dictionary\" that maps non-standard ids to a standard ones. Then this \"dictionary\" will be used to replace the mispelled artists.\n",
    "\n",
    "#### Question 4.1\n",
    "<div class=\"alert alert-info\">\n",
    "From data in the dataframe loaded from `/datasets/lastfm/artist_alias.txt`, construct a dictionary that maps each non-standard id to its standard id.  \n",
    "\n",
    "<div class=\"label label-success\">HINT:</div> Instead of using function `collect`, we can use `collectAsMap` to convert the collected data to a dictionary inline.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistAlias = (artistAliasDF\n",
    "                .rdd\n",
    "                .map(lambda row: ( row[\"MispelledArtistID\"], \n",
    "                                   row[\"StandardArtistID\"]))\n",
    "                .collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    We can store the aliases with a key-value pair data structure: this is pretty optimized for the current situation as we won't read all the values linearly. Then the reading of a value with the <q>MispelledArtistID</q> key will be in O(1). As far as we will access all the keys in a random order this will save a lot of time and computing power.\n",
    "    <br>\n",
    "    Furthermore, the transformation to a disctionnary is distributedly made and takes advantage of the four workers.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.2\n",
    "<div class=\"alert alert-info\">\n",
    "Using the constructed dictionary in question 4.1, replace the non-standard artist ids in the dataframe that was loaded from `/datasets/lastfm/user_artist_data.txt` by the corresponding standard ids then show 5 samples.\n",
    "</div>\n",
    "\n",
    "\n",
    "**NOTE 1**: If an id doesn't exist in the dictionary as a mispelled id, it is really a standard id.\n",
    "\n",
    "\n",
    "Using funtion `map` on Spark Dataframe will give us an RDD. We can convert this RDD back to Dataframe by using `sqlContext.createDataFrame(rdd_name, sql_schema)`\n",
    "\n",
    "\n",
    "**NOTE 2**: be careful! you need to be able to verify that you indeed solved the problem of having bad artist IDs. In principle, for the new data to be correct, we should to have duplicate pairs (user, artist), potentially with different play counts, right? In answering the question, please **show** that you indeed fixed the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of artist aliases is 190892.\n",
      "The number of attractors in the artist aliases is 22444.\n"
     ]
    }
   ],
   "source": [
    "loops_number = 20\n",
    "\n",
    "for _ in range(loops_number):\n",
    "    for k, v in artistAlias.items():\n",
    "        artistAlias[k] = artistAlias.get(v, v)\n",
    "        \n",
    "print(\"The number of artist aliases is {}.\"\n",
    "      .format(len(artistAlias.values())))\n",
    "print(\"The number of attractors in the artist aliases is {}.\"\n",
    "      .format(len(set(artistAlias.values()))))                                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    The loop ensures that the chained aliases will also be processed. However this is not a satisfying solution as the map function is computationnaly heavy. We will see later (part 4) in this notebook a better solution that uses DAGs to deal with that issue.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+\n",
      "| userID|artistID|playCount|\n",
      "+-------+--------+---------+\n",
      "|1000002|       1|       55|\n",
      "|1000002| 1000006|       33|\n",
      "|1000002| 1000007|        8|\n",
      "|1000002| 1000009|      144|\n",
      "|1000002| 1000010|      314|\n",
      "+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The script takes 1.611 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "def replaceMispelledIDs(fields):\n",
    "    # dict.get(a,b) returns dict[a] if found, else b\n",
    "    # Here it returns the original artist ID if mispelled, \n",
    "    # and if this is the orginal artist ID, it keeps it.\n",
    "    finalID = artistAlias.get(fields[1], fields[1])\n",
    "    return (fields[0], finalID, fields[2])\n",
    "\n",
    "t0 = time()\n",
    "newUserArtistDF = sqlContext.createDataFrame(\n",
    "    # Takes advantage of the 4 workers\n",
    "    userArtistDF.rdd.map(replaceMispelledIDs),\n",
    "    userArtistDataSchema\n",
    ")\n",
    "newUserArtistDF.show(5)\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "print(\"The script takes {:.3f} seconds\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+\n",
      "| userID|artistID|count|\n",
      "+-------+--------+-----+\n",
      "|2287002| 1018768|    4|\n",
      "|2287049| 1005039|    2|\n",
      "|2287764| 1052332|    2|\n",
      "|2288269| 1008824|    2|\n",
      "|2288457| 2003588|    2|\n",
      "+-------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "The script takes 84.416 seconds\n"
     ]
    }
   ],
   "source": [
    "# Check that we solved the problem\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "(newUserArtistDF\n",
    "     # We don't want duplicates with the same number of playCount for each key\n",
    "     .groupby(\"userID\", \"artistID\")\n",
    "     # Then we compute the number of entries for each key\n",
    "     .count()\n",
    "     # Finaly we display only the keys for which the keys have been duplicated\n",
    "     .filter(\"count(playcount) >= 2\")\n",
    "     .show(5))\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "print(\"The script takes {:.3f} seconds\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    Some entries are duplicated with a different playCount, which is an evidence that mispelled artists now have the right artistID.\n",
    "</p>\n",
    "<p>\n",
    "    This cell takes some time to compute as the data need to be reshuffled (the key sorting changes, and the network is indeed a bottleneck)\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.3\n",
    "<div class=\"alert alert-info\">\n",
    "Spark actions are executed through a set of stages, separated by distributed \"shuffle\" operations. Spark can be instructed to **automatically and efficiently** broadcast common data needed by tasks within **each stage**. The data broadcasted this way is cached in **serialized form** and deserialized before running each task.   \n",
    "<ul> </ul>\n",
    "We can thus improve our answer to question 4.2: we can reduce the communication cost by shipping the \"dictionary\" in a more efficient way by using `broadcast variable`. Broadcast variables allow the programmer to keep a read-only variable cached on **each machine** rather than shipping a copy of it with tasks. They are cached in deserialized form. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner.   \n",
    "<ul></ul>\n",
    "The broadcast of variable `v` can be created by `bV = sc.broadcast(v)`. Then value of this broadcast variable can be access via `bV.value`  \n",
    "\n",
    "<ul></ul>\n",
    "To question is then: using a broadcast variable, modify the script in question 4.2 to get better performance in terms of running time.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+\n",
      "| userID|artistID|playCount|\n",
      "+-------+--------+---------+\n",
      "|1000002|       1|       55|\n",
      "|1000002| 1000006|       33|\n",
      "|1000002| 1000007|        8|\n",
      "|1000002| 1000009|      144|\n",
      "|1000002| 1000010|      314|\n",
      "+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "The script takes 0.254 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "bArtistAlias = sc.broadcast(artistAlias)\n",
    "\n",
    "def replaceMispelledIDs(fields):\n",
    "    finalID = bArtistAlias.value.get(fields[1], fields[1])\n",
    "    return (fields[0], finalID, fields[2])\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "newUserArtistDF = sqlContext.createDataFrame(\n",
    "                    userArtistDF.rdd.map(replaceMispelledIDs), \n",
    "                    userArtistDataSchema\n",
    "                  )\n",
    "print(newUserArtistDF.show(5))\n",
    "t1 = time()\n",
    "\n",
    "print(\"The script takes {:.3f} seconds\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "Broadcasting variables is way faster than accessing it each time from the driver. The network is a huge bottleneck in the previous implementation.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although having some advantages, explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Well, our data frame contains clean and \"standard\" data. We can use it to redo previous statistic queries.\n",
    "\n",
    "#### Question 5.1\n",
    "<div class=\"alert alert-info\">\n",
    "How many unique artists? Compare with the result when using old data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total n. of artists:  1568117\n"
     ]
    }
   ],
   "source": [
    "# Previous unique artists number (without correcting aliases)\n",
    "mispelledUniqueArtists = uniqueArtists\n",
    "\n",
    "# Computing the new count\n",
    "uniqueArtists = newUserArtistDF.select(\"artistID\").distinct().count()\n",
    "\n",
    "print(\"Total n. of artists: \", uniqueArtists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was 62911 non correct artist entries!\n",
      "(3.86 % of the artist entries)\n"
     ]
    }
   ],
   "source": [
    "print(\"There was {} non correct artist entries!\"\n",
    "          .format(mispelledUniqueArtists - uniqueArtists)\n",
    "      + \"\\n\" +\n",
    "      \"({:.2f} % of the artist entries)\"\n",
    "          .format(100*(mispelledUniqueArtists - uniqueArtists)/mispelledUniqueArtists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "The number of artists decreases once the mispelling is corrected. This will lead to better results in recommendation due to a less sparse matrix and correct number of listening in it.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.2\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Who are the top-10 artists?\n",
    "<ul>\n",
    "  <li>In terms of absolute play counts</li>\n",
    "  <li>In terms of \"audience size\", that is, how many users listened to one of their track at least once</li>\n",
    "</ul>  \n",
    "\n",
    "Plot the results, and explain the figures you obtain.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate top-10 artists in term of play counts\n",
    "top10ArtistsPC = (\n",
    "                    newUserArtistDF\n",
    "                        .groupBy( \"artistID\" )\n",
    "                        .sum( \"playCount\" )\n",
    "                        .orderBy('sum(playCount)', ascending=0)\n",
    "                        .take(10)\n",
    "                  )\n",
    "\n",
    "y_pos = range(len(top10ArtistsPC))\n",
    "pdf = pd.DataFrame(data=top10ArtistsPC)\n",
    "\n",
    "top10PC_set = set(pdf[0])\n",
    "\n",
    "plt.barh(y_pos, pdf[1][::-1], align='center', alpha=0.4)\n",
    "plt.yticks(y_pos, pdf[0][::-1])\n",
    "plt.xlabel('Play Count')\n",
    "plt.ylabel('Artist')\n",
    "plt.title('Top-10 Artist ID per play counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate top-10 artists in term of audience size\n",
    "top10ArtistsASS = (\n",
    "                    newUserArtistDF\n",
    "                        .select([\"userID\", \"artistID\"])\n",
    "                        .drop_duplicates()\n",
    "                        .groupBy( \"artistID\" )\n",
    "                        .count()\n",
    "                        .orderBy('count', ascending=0)\n",
    "                        .take(10)\n",
    "                  )\n",
    "\n",
    "y_pos = range(len(top10ArtistsASS))\n",
    "pdf = pd.DataFrame(data=top10ArtistsASS)\n",
    "\n",
    "top10ASS_set = set(pdf[0])\n",
    "\n",
    "plt.barh(y_pos, pdf[1][::-1], align='center', alpha=0.4)\n",
    "plt.yticks(y_pos, pdf[0][::-1])\n",
    "plt.xlabel('Audience size')\n",
    "plt.ylabel('Artist')\n",
    "plt.title('Top-10 Artist ID per audience size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artists that are in both top10 \n",
    "\n",
    "top10PC_set & top10ASS_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "5 Artists are in both of the top ten ranking. Those artists benefits from a large audience which increases their playCount. The other artists in the top 10 for playCounts are more likely to be loop-listened.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.3\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Who are the top-10 users?\n",
    "<ul>\n",
    "  <li>In terms of absolute play counts</li>\n",
    "  <li>In terms of \"curiosity\", that is, how many different artists they listened to</li>\n",
    "\n",
    "</ul>  \n",
    "\n",
    "Plot the results\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate top 10 users interm of play counts\n",
    "top10UsersByPlayCount = (\n",
    "                        newUserArtistDF\n",
    "                             .groupBy( \"userID\" )\n",
    "                             .sum( \"playCount\" )\n",
    "                             .orderBy('sum(playCount)', ascending=0)\n",
    "                             .take(10)\n",
    "                        )\n",
    "\n",
    "y_pos = range(len(top10UsersByPlayCount))\n",
    "pdf = pd.DataFrame(data=top10UsersByPlayCount)\n",
    "\n",
    "top10PC_set = set(pdf[0])\n",
    "\n",
    "plt.barh(y_pos, pdf[1][::-1], align='center', alpha=0.4)\n",
    "plt.yticks(y_pos, pdf[0][::-1])\n",
    "plt.xlabel('Play Count')\n",
    "plt.ylabel('User')\n",
    "plt.title('Top-10 Users ID per play counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    These users are likely to play music the whole day (and night) without beeing its time all the time. These may be professionnals (ambiance listening) or users that forgot their music playing pretty often.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate top 10 users interm of curiosity\n",
    "top10UsersByCuriosity = (\n",
    "                        newUserArtistDF\n",
    "                            .groupBy(\"userID\")\n",
    "                            .count()\n",
    "                            .orderBy('count', ascending=0)\n",
    "                            .take(10)\n",
    "                        )\n",
    "\n",
    "y_pos = range(len(top10UsersByCuriosity))\n",
    "pdf = pd.DataFrame(data=top10UsersByCuriosity)\n",
    "\n",
    "top10cur_set = set(pdf[0])\n",
    "\n",
    "plt.barh(y_pos, pdf[1][::-1], align='center', alpha=0.4)\n",
    "plt.yticks(y_pos, pdf[0][::-1])\n",
    "plt.xlabel('Different artists count')\n",
    "plt.ylabel('User')\n",
    "plt.title('Top-10 Users ID per curiosity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users that are in both top10 \n",
    "\n",
    "top10PC_set & top10cur_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    No user is in the curiosity top 10 and the play count top 10, which seems legit if we consider the play count top 10 as not very concentrated on their musical choices (which is obviously the case, with regard to their play count).\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some valuable information about the data. It's the time to study how to build a statistical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build a statistical models to make recommendations\n",
    "\n",
    "## 2.1 Introduction to recommender systems\n",
    "\n",
    "In a recommendation-system application there are two classes of entities, which we shall refer to as `users` and `items`. Users have preferences for certain items, and these preferences must be inferred from the data. The data itself is represented as a `preference matrix` $A$, giving for each user-item pair, a value that represents what is known about the degree of preference of that user for that item. The table below is an example for a `preference matrix` of 5 users and `k` items. The `preference matrix` is also known as `utility matrix`.\n",
    "\n",
    "| | IT1 | IT2 | IT3 | ... | ITk |\n",
    "|---|---|---|---|---|---|\n",
    "| U1 | 1 |  | 5 | ... | 3 |\n",
    "| U2 |  | 2 |  | ... | 2 |\n",
    "| U3 | 5 |  | 3 | ... |  |\n",
    "| U4 | 3 | 3 |  | ... | 4 |\n",
    "| U5 |  | 1 |  | ... | ... |\n",
    "\n",
    "The value of row i, column j expresses how much does user `i` like item `j`. The values are often the rating scores of users for items. An unknown value implies that we have no explicit information about the user's preference for the item. The goal of a recommendation system is to predict \"the blanks\" in the `preference matrix`. For example, assume that the rating score is from 1 (dislike) to 5 (love), would user `U5` like `IT3` ? We have two approaches:\n",
    "\n",
    "* Designing our recommendation system to take into account properties of items such as brand, category, price... or even the similarity of their names. We can denote the similarity of items `IT2` and `IT3`, and then conclude that because user `U5` did not like `IT2`, they were unlikely to enjoy SW2 either.\n",
    "\n",
    "* We might observe that the people who rated both `IT2` and `IT3` tended to give them similar ratings. Thus, we could conclude that user `U5` would also give `IT3` a low rating, similar to `U5`'s rating of `IT2`\n",
    "\n",
    "It is not necessary to predict every blank entry in a `utility matrix`. Rather, it is only necessary to discover some entries in each row that are likely to be high. In most applications, the recommendation system does not oﬀer users a ranking of all items, but rather suggests a few that the user should value highly. It may not even be necessary to ﬁnd all items with the highest expected ratings, but only to ﬁnd a large subset of those with the highest ratings.\n",
    "\n",
    "\n",
    "## 2.2 Families of recommender systems\n",
    "\n",
    "In general, recommender systems can be categorized into two groups:\n",
    "\n",
    "* **Content-Based** systems focus on properties of items. Similarity of items is determined by measuring the similarity in their properties.\n",
    "\n",
    "* **Collaborative-Filtering** systems focus on the relationship between users and items. Similarity of items is determined by the similarity of the ratings of those items by the users who have rated both items.\n",
    "\n",
    "In the usecase of this notebook, artists take the role of `items`, and `users` keep the same role as `users`.\n",
    "Since we have no information about `artists`, except their names, we cannot build a `content-based` recommender system.\n",
    "\n",
    "Therefore, in the rest of this notebook, we only focus on `Collaborative-Filtering` algorithms.\n",
    "\n",
    "## 2.3 Collaborative-Filtering \n",
    "In this section, we study a member of a broad class of algorithms called `latent-factor` models. They try to explain observed interactions between large numbers of users and products through a relatively small number of unobserved, underlying reasons. It is analogous to explaining why millions of people buy a particular few of thousands of possible albums by describing users and albums in terms of tastes for perhaps tens of genres, tastes which are **not directly observable or given** as data. \n",
    "\n",
    "First, we formulate the learning problem as a matrix completion problem. Then, we will use a type of `matrix factorization` model to \"fill in\" the blanks.  We are given implicit ratings that users have given certain items (that is, the number of times they played a particular artist) and our goal is to predict their ratings for the rest of the items. Formally, if there are $n$ users and $m$ items, we are given an $n \\times m$ matrix $R$ in which the generic entry $(u, i)$ represents the rating for item $i$ by user $u$. **Matrix $R$ has many missing entries indicating unobserved ratings, and our task is to estimate these unobserved ratings**.\n",
    "\n",
    "A popular approach to the matrix completion problem is **matrix factorization**, where we want to \"summarize\" users and items with their **latent factors**.\n",
    "\n",
    "### 2.3.1 Basic idea and an example of Matrix Factorization\n",
    "For example, given a preference matrix 5x5 as below, we want to approximate this matrix into the product of two smaller matrixes $X$ and $Y$ .\n",
    "\n",
    "$$\n",
    "M = \n",
    "\\begin{bmatrix}\n",
    " 5 & 2 & 4 & 4 & 3 \\\\\n",
    " 3 & 1 & 2 & 4 & 1 \\\\\n",
    " 2 &  & 3 & 1 & 4 \\\\\n",
    " 2 & 5 & 4 & 3 & 5 \\\\\n",
    " 4 & 4 & 5 & 4 &  \\\\\n",
    "\\end{bmatrix}\n",
    "\\approx M^\\prime =\n",
    "\\begin{bmatrix}\n",
    " x_{11} & x_{12} \\\\\n",
    " x_{21} & x_{22} \\\\\n",
    " x_{31} & x_{32} \\\\\n",
    " x_{41} & x_{42} \\\\\n",
    " x_{51} & x_{52} \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    " y_{11} & y_{12} & y_{13} & y_{14} & y_{15} \\\\\n",
    " y_{21} & y_{22} & y_{23} & y_{24} & y_{25} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$M^\\prime$ is an approximation that is as close to A as possible. To calculate how far from $M$ $M^\\prime$ is, we often calculate the sum of squared distances of non-empty elements in $M$ and the corresponding elements in $M^\\prime$.\n",
    "In this way, for $M^\\prime$, besides the approximated elements in $M$, we also have the non-observed elements. Therefore, to see how much does user `i` like item `j`, we simply pick up the value of $M^\\prime_{i,j}$.\n",
    "\n",
    "The challenge is how to calculate $X$ and $Y$. The bad news is that this can't be solved directly for both the best $X$ and best $Y$ at the same time. Fortunately, if $Y$ is known, we can calculate the best of $X$, and vice versa. It means from the initial values of $X$ and $Y$ in the beginning, we calculate the best $X$ according to $Y$, and then calculate the best $Y$ according to the new $X$. This process is repeated until the distance from $XY$ to $M$ is small. It's simple, right ?\n",
    "\n",
    "Let's take an example. To compute the approximation for the above 5x5 matrix $M$, first, we initialize the value of $X$ and $Y$ as below.\n",
    "\n",
    "$$\n",
    "M^\\prime = X \\times Y =\n",
    "\\begin{bmatrix}\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With the initial iteration, we calculate the the Root-Mean-Square Error from $XY$ to $M$.\n",
    "\n",
    "Consider the ﬁrst rows of $M$ and $XY$ . We subtract the first row of $XY$ from the entries in the ﬁrst row of $M$, to get $3,0,2,2,1$. We square and sum these to get $18$. \n",
    "\n",
    "In the second row, we do the same to get $1,−1,0,2,−1$, square and sum to get $7$. \n",
    "\n",
    "In the third row, the second column is blank, so that entry is ignored when computing the RMSE. The diﬀerences are $0,1,−1,2$ and the sum of squares is $6$. \n",
    "\n",
    "For the fourth row, the diﬀerences are $0,3,2,1,3$ and the sum of squares is $23$. \n",
    "\n",
    "The ﬁfth row has a blank entry in the last column, so the diﬀerences are $2,2,3,2$ and the sum of squares is $21$. \n",
    "\n",
    "When we sum the sums from each of the ﬁve rows, we get $18+7+6+23+21 = 75$. So, $RMSE=\\sqrt{75/23}=1.806$ where $23$ is the number of non-empty values in $M$.\n",
    "\n",
    "Next, with the given value of $Y$, we calculate $X$ by finding the best value for $X_{11}$.\n",
    "\n",
    "$$\n",
    "M^\\prime = X \\times Y =\n",
    "\\begin{bmatrix}\n",
    " x & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " x+1 & x+1 & x+1 & x+1 & x+1 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, to minimize the $RMSE$  we minimize the difference of the first rows $(5−(x+1))^2 + (2−(x+1))^2 + (4−(x+1))^2 + (4−(x+1))^2 + (3−(x+1))^2$. By  taking the derivative and set that equal to 0, we pick $x=2.6$\n",
    "\n",
    "Given the new value of $X$, we can calculate the best value for $Y$.\n",
    "\n",
    "$$\n",
    "M^\\prime = X \\times Y =\n",
    "\\begin{bmatrix}\n",
    " 2.6 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    " y & 1 & 1 & 1 & 1 \\\\\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " 3.6 & 3.6 & 3.6 & 3.6 & 3.6 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "By doing the same process as before, we can pick value for $y=1.617$. After that, we can check if the $RMSE$ is not converged, we continue to update $X$ by $Y$ and vice versa. In this example, for simple, we only update one element of each matrix in each iteration. In practice, we can update a full row or full matrix at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Matrix Factorization: Objective and ALS Algorithm on a Single Machine\n",
    "\n",
    "More formally, in general, we select $k$ latent features, and describe each user $u$ with a $k-$dimensional vector $x_u$, and each item $i$ with a $k-$dimensional vector $y_i$.\n",
    "\n",
    "Then, to predict user $u$'s rating for item $i$, we do as follows: $ r_{ui} \\approx x_{u}^{T}y_i$.\n",
    "\n",
    "This can be put, more elegantly, in a matrix form. Let $x_1, \\cdots x_n \\in \\mathbb{R}^k$ be the factors for the users, and $y_1, \\cdots y_m \\in \\mathbb{R}^k$ the factors for the items. The $k \\times n$ user matrix $X$ and the $k \\times m$ item matrix $Y$ are then defined by:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    " |   &         & |  \\\\\n",
    "x_1  &  \\cdots & x_n\\\\\n",
    " |   &         & |  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    " |   &         & |  \\\\\n",
    "y_1  &  \\cdots & y_i\\\\\n",
    " |   &         & |  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Our goal is to estimate the complete ratings matrix $R \\approx X^{T} Y$. We can formulate this problem as an optimization problem in which we aim to minimize an objective function and find optimal $X$ and $Y$ . In particular, we aim to minimize the least squares error of the observed ratings (and regularize):\n",
    "\n",
    "$$\n",
    "\\min_{X,Y} \\sum_{r_{ui} \\text{observed}}(r_{ui} - x_{u}^{T}y_i)^2 + \\lambda \\left( \\sum_{u} \\|x_u\\|^2 + \\sum_{i} \\|y_i\\|^2 \\right) \n",
    "$$\n",
    "\n",
    "Notice that this objective is non-convex (because of the $x_{u}^{T} y_i$ term); in fact it’s NP-hard to optimize. Gradient descent can be used as an approximate approach here, however it turns out to be slow and costs lots of iterations. Note however, that if we fix the set of variables $X$ and treat them as constants, then the objective is a convex function of $Y$ and vice versa. Our approach will therefore be to fix $Y$ and optimize $X$, then fix $X$ and optimize $Y$, and repeat until convergence. This approach is known as **ALS (Alternating Least Squares)**. For our objective function, the alternating least squares algorithm can be expressed with this simple pseudo-code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Initialize** $X$, $Y$\n",
    "\n",
    "**while(convergence is not true) do**\n",
    "\n",
    "\n",
    "**for** $u = 1 \\cdots n$ **do**\n",
    "\n",
    "$x_u = \\left( \\sum_{r_ui \\in r_{u*}} y_i y_{i}^{T} + \\lambda I_k \\right)^{-1} \\sum_{r_ui \\in r_{u*}} r_{ui} y_i $ \n",
    "   \n",
    "**end for**\n",
    "\n",
    "**for** $u = 1 \\cdots n$ **do**\n",
    "\n",
    "$y_i = \\left( \\sum_{r_ui \\in r_{*i}} x_u x_{u}^{T} + \\lambda I_k \\right)^{-1} \\sum_{r_ui \\in r_{*i}} r_{ui} x_u $ \n",
    "   \n",
    "**end for**\n",
    "\n",
    "\n",
    "**end while**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single machine, we can analyze the computational cost of this algorithm. Updating each $x_u$ will cost $O(n_u k^2 + k^3)$, where $n_u$ is the number of items rated by user $u$, and similarly updating each $y_i$ will cost $O(n_i k^2 + k^3)$, where $n_i$ is the number of users that have rated item $i$.\n",
    "\n",
    "\n",
    "Once we’ve computed the matrices $X$ and $Y$, there are several ways compute a prediction. The first is to do what was discussed before, which is to simply predict $ r_{ui} \\approx x_{u}^{T}y_i$ for each user $u$ and item $i$. \n",
    "This approach will cost $O(nmk)$ if we’d like to estimate every user-item pair. \n",
    "\n",
    "However, this approach is prohibitively expensive for most real-world datasets. A second (and more holistic) approach is to use the $x_u$ and $y_i$ as features in another learning algorithm, incorporating these features with others that are relevant to the prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Parallel Altenating Least Squares\n",
    "\n",
    "There are several ways to distribute the computation of the ALS algorithm depending on how data is partitioned.\n",
    "\n",
    "#### Method 1: using joins\n",
    "First we consider a fully distributed version, in the sense that all data (both input and output) is stored in a distributed file system. In practice, input data (ratings) and parameters ($X$ and $Y$) are stored in an a Spark RDD. Specifically, ratings -- that are always **sparse** -- are stored as RDD of triplets:\n",
    "\n",
    "Ratings: RDD((u, i, $r_{ui}$), . . . )\n",
    "\n",
    "\n",
    "Instead, we can use dense representation for factor matrices $X$ and $Y$, and these are stored as RDDs of vectors. More precisely, we can use the data types introduced in Spark MLLib to store such vectors and matrices:\n",
    "\n",
    "X : RDD($x_1$,...,$x_n$)\n",
    "\n",
    "Y : RDD($y_1$,...,$y_m$)\n",
    "\n",
    "\n",
    "Now, recall the expression to compute $x_u$:\n",
    "\n",
    "$x_u = \\left( \\sum_{r_ui \\in r_{u*}} y_i y_{i}^{T} + \\lambda I_k \\right)^{-1} \\sum_{r_ui \\in r_{u*}} r_{ui} y_i $ \n",
    "\n",
    "Let's call the first summation *part A* and the second summation *part B*. To compute such parts, in parallel, we can proceed with the following high-level pseudocode:\n",
    "\n",
    "* Join the Ratings RDD with the $Y$ matrix RDD using key $i$ (items)\n",
    "* Map to compute $y_i y_{i}^{T}$ and emit using key $u$ (user)\n",
    "* ReduceByKey $u$ (user) to compute $\\sum_{r_ui \\in r_{u*}} y_i y_{i}^{T}$\n",
    "* Invert\n",
    "* Another ReduceByKey $u$ (user) to compute $\\sum_{r_ui \\in r_{u*}} r_{ui} y_i$\n",
    "\n",
    "We can use the same template to copmute $y_i$.\n",
    "\n",
    "This approach works fine, but note it requires computing $y_i y_{i}^{T}$ for each user that has rated item $i$. \n",
    "\n",
    "#### Method 2: using broadcast variables (advanced topic)\n",
    "The next approach takes advantage of the fact that the $X$ and $Y$ factor matrices are often very small and can be stored locally on each machine.\n",
    "\n",
    "* Partition the Ratings RDD **by user** to create $R_1$, and similarly partition the Ratings RDD **by item** to create $R_2$. This means there are two copies of the same Ratings RDD, albeit with different partitionings. In $R_1$, all ratings by the same user are on the same machine, and in $R_2$ all ratings for same item are on the same machine.\n",
    "* Broadcast the matrices $X$ and $Y$. Note that these matrices are not RDD of vectors: they are now \"local: matrices.\n",
    "* Using $R_1$ and $Y$, we can use expression $x_u$ from above to compute the update of $x_u$ locally on each machine\n",
    "* Using $R_2$ and $X$, we can use expression $y_i$ from above to compute the update of $y_i$ locally on each machine\n",
    "\n",
    "A further optimization to this method is to group the $X$ and $Y$ factors matrices into blocks (user blocks and item blocks) and reduce the communication by only sending to each machine the block of users (or items) that are needed to compute the updates at that machine. \n",
    "\n",
    "This method is called **Block ALS**. It is achieved by precomputing some information about the ratings matrix to determine the \"out-links\" of each user (which blocks of the items it will contribute to) and \"in-link\" information for each item (which of the factor vectors it receives from each user block it will depend on). For exmple, assume that machine 1 is responsible for users 1,2,...,37: these will be block 1 of users. The items rated by these users are block 1 of items. Only the factors of block 1 of users and block 1 of items will be broadcasted to machine 1.\n",
    "\n",
    "### Further readings\n",
    "Other methods for matrix factorization include:\n",
    "\n",
    "* Low Rank Approximation and Regression in Input Sparsity Time, by Kenneth L. Clarkson, David P. Woodruff. http://arxiv.org/abs/1207.6365\n",
    "* Generalized Low Rank Models (GLRM), by Madeleine Udell, Corinne Horn, Reza Zadeh, Stephen Boyd. http://arxiv.org/abs/1410.0342\n",
    "* Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares, by Trevor Hastie, Rahul Mazumder, Jason D. Lee, Reza Zadeh . Statistics Department and ICME, Stanford University, 2014. http://stanford.edu/~rezab/papers/fastals.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Usecase : Music recommender system\n",
    "\n",
    "In this usecase, we use the data of users and artists in the previous sections to build a statistical model to recommend artists for users.\n",
    " \n",
    "## 3.1 Requirements\n",
    "According to the properties of data, we need to choose a recommender algorithm that is suitable for this implicit feedback data. It means that the algorithm should learn without access to user or artist attributes such as age, genre,.... Therefore, an algorithm of type `collaborative filtering` is the best choice.\n",
    "\n",
    "Second, in the data, there are some users that have listened to only 1 artist. We need an algorithm that might provide decent recommendations to even these users. After all, at some point, every user starts out with just one play at some point! \n",
    "\n",
    "Third, we need an algorithm that scales, both in its ability to build large models, and to create recommendations quickly. So, an algorithm which can run on a distributed system (SPARK, Hadoop...) is very suitable.\n",
    "\n",
    "From these requirement, we can choose using ALS algorithm in SPARK's MLLIB.\n",
    "\n",
    "Spark MLlib’s ALS implementation draws on ideas from [1](http://yifanhu.net/PUB/cf.pdf) and [2](http://link.springer.com/chapter/10.1007%2F978-3-540-68880-8_32).\n",
    "\n",
    "## 3.2 Notes\n",
    "\n",
    "Currently, MLLIB can only build models from an RDD. That means we have two ways to prepare data:\n",
    "\n",
    "* Loading to into SPARK SQL DataFrame as before, and then access the corresponding RDD by calling `<dataframe>.rdd`. The invalid data is often sucessfully dropped by using mode `DROPMALFORMED`. However, this way might not work in all cases. Fortunately, we can use it with this usecase.\n",
    "\n",
    "* Loading data directly to RDD. However, we have to deal with the invalid data ourself. In the trade-off, this way is the most reliable, and can work in every case.\n",
    "\n",
    "In this notebook, we will use the second approach: it requires a bit more effort, but the reward is worth it!\n",
    "\n",
    " \n",
    "## 3.3 Cleanup the data\n",
    "In section 1, we already replaced the ids of mispelled artists by the corresponding standard ids by using SPARK SQL API.\n",
    "However, if the data has the invalid entries such that SPARK SQL API is stuck, the best way to work with it is using an RDD.\n",
    "\n",
    "Just as a recall, we work with three datasets in `user_artist_data.txt`, `` and `artist_alias.txt`. The entries in these file can be empty or have only one field. \n",
    "\n",
    "In details our goal now is:\n",
    "\n",
    "* Read the input ```user_artist_data.txt``` and transforms its representation into an output dataset.\n",
    "* To produce an output \"tuple\" containing the original user identifier and play counts, but with the artist identifier replaced by its most common alias, as found in the ```artist_alias.txt``` dataset.\n",
    "* Since the ```artist_alias.txt``` file is small, we can use a technique called **broadcast variables** to make such transformation more efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "#### Question 6.1\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Load data from `/datasets/lastfm/artist_alias.txt` and filter out the invalid entries to construct a dictionary to map from mispelled artists' ids to standard ids.\n",
    "</div>\n",
    "\n",
    "NOTE: From now on, we will use the \"standard\" data to train our model.\n",
    "\n",
    "HINT: If a line contains less than 2 fields or contains invalid numerial values, we can return a special tuple. After that, we can filter out these special tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawArtistAlias = sc.textFile(base + \"artist_alias.txt\")\n",
    "\n",
    "def xtractFields(s):\n",
    "    # Using white space or tab character as separators,\n",
    "    # split a line into list of strings \n",
    "    line = re.split(\"\\s|\\t\",s,1)\n",
    "    # if this line has at least 2 characters\n",
    "    if (len(line) > 1):\n",
    "        try:\n",
    "            # try to parse the first and the second components to integer type\n",
    "            return (int(line[0]), int(line[1]))\n",
    "        except ValueError:\n",
    "            # if parsing has any error, return a special tuple\n",
    "            return (-1,-1)\n",
    "    else:\n",
    "        # if this line has less than 2 characters, return a special tuple\n",
    "        return (-1,-1)\n",
    "\n",
    "artistAlias = (\n",
    "                rawArtistAlias\n",
    "                    # extract fields using function xtractFields\n",
    "                    .map( xtractFields )\n",
    "    \n",
    "                    # filter out the special tuples\n",
    "                    .filter( lambda x:x!=(-1,-1) )\n",
    "    \n",
    "                    # collect result to the driver as a \"dictionary\"\n",
    "                    .collectAsMap()\n",
    "                )\n",
    "\n",
    "# Preventing alias-chains\n",
    "for _ in range(loops_number):\n",
    "    for k, v in artistAlias.items():\n",
    "        artistAlias[k] = artistAlias.get(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6.2\n",
    "<div class=\"alert alert-info\">\n",
    "Using the dictionary in question 6.1, prepare RDD `userArtistDataRDD` by replacing mispelled artists' ids to standard ids. Show 5 samples.\n",
    "</div>\n",
    "\n",
    "HINT: Using broadcast varible can help us increase the effiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bArtistAlias = sc.broadcast(artistAlias)\n",
    "rawUserArtistData = sc.textFile(base + \"user_artist_data.txt\")\n",
    "\n",
    "def disambiguate(line):\n",
    "    [userID, artistID, count] = line.split(' ')\n",
    "    finalArtistID = bArtistAlias.value.get(artistID, artistID)\n",
    "    return (userID, finalArtistID, count)\n",
    "\n",
    "userArtistDataRDD = rawUserArtistData.map(disambiguate)\n",
    "userArtistDataRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Use the previously cleaned data \n",
    "\n",
    "cleanUserArtistDF.rdd.collect(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Training our statistical model\n",
    "To train a model using ALS, we must use a preference matrix  as an input. MLLIB uses the class `Rating` to support the construction of a distributed preference matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "#### Question 7.1\n",
    "<div class=\"alert alert-info\">\n",
    "Given RDD `userArtistDataRDD` in question 6.2, construct a new RDD `trainingData` by tranforming each item of it into a `Rating` object.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after replacing mispelled artist ids, the data contains some play count duplications of a user with the same artist\n",
    "# remember to aggregate these records\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "allData = (\n",
    "            userArtistDataRDD\n",
    "                # Map-Reduce implementation to remove play count duplications\n",
    "                # Transforms data to key-value pairs (key=(UserID, ArtistID) , value=playCount)\n",
    "                .map(lambda x: ((x[0], x[1]),x[2]))\n",
    "                # Aggregates the duplicated playCount by summing them\n",
    "                .reduceByKey(sum)\n",
    "                # Use the right datastructure for recommendation algorithms, \n",
    "                # which is the Rating object in Spark\n",
    "                .map(lambda r: Rating(r[0][0], r[0][1], r[1]))\n",
    "                # Set the repartition to 4 datanodes to split the data into our 4 workers\n",
    "                .repartition(numPartitions=4)\n",
    "                # Cache the dataset\n",
    "                .cache()\n",
    "            )\n",
    "             \n",
    "print(\"\\n\".join(map(repr,allData.take(5))))\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "print(\"The script takes {:.3f} seconds\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "    There are 4 workers, therefore the repartition number is set to 4 which means that each worker is supposed to have an equal workload while processing the data (reparted equally between them).\n",
    "</p>\n",
    "<p>\n",
    "    Here, the main bottleneck is the <code>reduceByKey</code> function which takes about 3/4 of the overall running time. This is partly due to the huge number of keys here.\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7.2\n",
    "<div class=\"alert alert-info\">\n",
    "A model can be trained by using `ALS.trainImplicit(<training data>, <rank>)`, where:\n",
    "<ul>\n",
    "<li>`training data` is the input data you decide to feed to the ALS algorithm</li>\n",
    "<li>`rank` is the number of laten features</li>\n",
    "</ul>  \n",
    "\n",
    "\n",
    "We can also use some additional parameters to adjust the quality of the model. Currently, let's set \n",
    "<ul>\n",
    "<li>`rank=10`</li>\n",
    "<li>`iterations=5`</li>\n",
    "<li>`lambda_=0.01`</li>\n",
    "<li>`alpha=1.0` </li>\n",
    "</ul>\n",
    "to build model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "model = ALS.trainImplicit(allData,\n",
    "                            rank=10,\n",
    "                            iterations=5,\n",
    "                            lambda_=0.01,\n",
    "                            alpha=1.0)\n",
    "t1 = time()\n",
    "print(\"finish training model in {:.3f} secs\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few precisions on the `trainImplicit` function\n",
    "<div class=\"alert alert-success\">\n",
    "<p class=\"alert alert-info\">\n",
    "<q>Essentially, instead of trying to model the matrix of ratings directly, this approach treats the data as numbers representing the strength in observations of user actions (such as the number of clicks, or the cumulative duration someone spent viewing a movie). Those numbers are then related to the level of confidence in observed user preferences, rather than explicit ratings given to items. The model then tries to find latent factors that can be used to predict the expected preference of a user for an item.</q>\n",
    "<br>\n",
    "<b>Source:</b> <a href=\"https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html\">Documentation link</a>\n",
    "</p>\n",
    "<p>\n",
    "    This is in fact particularly adapted to the model we use to predict artists to users as the play count is not directly related to any rate.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7.3\n",
    "<div class=\"alert alert-info\">\n",
    "The trained model can be saved into HDFS for later use. This can be done via `model.save(sc, <file_name>)`.\n",
    "Let's use this function to store our model as name `lastfm_model.spark`.\n",
    "</div>\n",
    "\n",
    "NOTE 1: since you may have noticed that building the model takes some time, it might come to your mind that this information could be stored, such that you can \"interrupt\" your laboratory session here, and restart next time by loading your model.\n",
    "\n",
    "NOTE 2: funnily enough, it could take more time to save the model than to build it from scratch! So take a look at the execution time to save the model: this method actually stores the model as Parquet files, which are column-oriented and compressed.\n",
    "\n",
    "NOTE 3: to check you have your file on HDFS, you are invited to open a terminal from the \"Home\" Jupyter dashboard, and type `hdfs dfs -ls` to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -rm -R -f -skipTrash lastfm_model.spark\n",
    "t0 = time()\n",
    "model.save( sc , \"lastfm_model.spark\")\n",
    "t1 = time()\n",
    "print(\"finish loading model in {:.3f} secs\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7.4\n",
    "<div class=\"alert alert-info\">\n",
    "A saved model can be load from file by using `MatrixFactorizationModel.load(sc, <file_name>)`. \n",
    "\n",
    "Let's load our model from file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "model = MatrixFactorizationModel.load(sc, \"lastfm_model.spark\")\n",
    "t1 = time()\n",
    "print(\"finish loading model in {:.3f} secs\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7.5\n",
    "<div class=\"alert alert-info\">\n",
    "Print the first row of user features in our model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.userFeatures().first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "<div class=\"alert alert-info\">\n",
    "Show the top-5 artist names recommendated for a given user, for example: `2093760` (please, try with different users!).\n",
    "</div>\n",
    "\n",
    "HINT: The recommendations can be given by function `recommendProducts(userID, num_recommendations)`. These recommendations are only artist ids. You have to map them to artist names by using data in `artist_data.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful functions\n",
    "\n",
    "def artistIDtoName(artistID_set):\n",
    "    \"\"\"\n",
    "        This function  convets a set of artist IDs \n",
    "        into a corresponding list of artist names.\n",
    "    \"\"\"\n",
    "    return artistByID \\\n",
    "        .filter(lambda line: line[0] in artistID_set) \\\n",
    "        .values() \\\n",
    "        .collect()\n",
    "\n",
    "\n",
    "def print_recommendations(userID, recommendationsID):\n",
    "    \"\"\"\n",
    "        Print the recommendations for userID.\n",
    "        recommendationsID is a set of artist IDs.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n User {} top-{} artist recommendations: \\n\"\n",
    "              .format(userID, len(recommendationsID))\n",
    "          +\n",
    "          \"\\n\".join(artistIDtoName(recommendationsID)))\n",
    "\n",
    "    \n",
    "def recommend_from_model(model, userID, n_recommendations = 5):\n",
    "    \"\"\"\n",
    "        This takes a trained model, an userID and \n",
    "        returns a set of n_recommendations artist IDs \n",
    "        that are most recommended by this model.\n",
    "    \"\"\"\n",
    "    \n",
    "    recommendations = model.recommendProducts(userID, n_recommendations)\n",
    "    recArtist = { x.product for x in recommendations } \n",
    "    return recArtist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make five reccommendations to user 2093760\n",
    "recommendations = model.recommendProducts(2093760, 5)\n",
    "\n",
    "# construct set of recommendated artists (x.product)\n",
    "recArtist = { x.product for x in recommendations } # {} is equivalent to set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct data of artists (artist_id, artist_name)\n",
    "\n",
    "rawArtistData = sc.textFile(base + \"artist_data.txt\")\n",
    "\n",
    "def xtractFields(s):\n",
    "    line = re.split(\"\\s|\\t\",s,1)\n",
    "    if (len(line) > 1):\n",
    "        try:\n",
    "            return (int(line[0]), str(line[1].strip()))\n",
    "        except ValueError:\n",
    "            return (-1,\"\")\n",
    "    else: \n",
    "        return (-1,\"\")\n",
    "\n",
    "artistByID = rawArtistData.map(xtractFields).filter(lambda x: x[0] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_recommendations(2093760, recArtist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with some example users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIDs = [1000002, 2152260, 2377755]\n",
    "\n",
    "for userID in userIDs:\n",
    "    print_recommendations(userID, \n",
    "                          recommend_from_model(\n",
    "                              model, \n",
    "                              userID, \n",
    "                              n_recommendations = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "At the moment, it is necessary to manually unpersist the RDDs inside the model when you are done with it. The following function can be used to make sure models are promptly uncached.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpersist(model):\n",
    "    model.userFeatures().unpersist()\n",
    "    model.productFeatures().unpersist()\n",
    "\n",
    "# uncache data and model when they are no longer used  \n",
    "unpersist(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Evaluating Recommendation Quality \n",
    "\n",
    "In this section, we study how to evaluate the quality of our model. It's hard to say how good the recommendations are.\n",
    "One of serveral methods approach to evaluate  a recommender based on its ability to rank good items (artists) high in a list of recommendations. The problem is how to define \"good artists\". Currently, by training all data, \"good artists\" is defined as \"artists the user has listened to\", and the recommender system has already received all of this information as input. It could trivially return the users previously-listened artists as top recommendations and score perfectly. Indeed, this is not useful, because the recommender's is used to recommend artists that the user has **never** listened to. \n",
    "\n",
    "To overcome that problem, we can hide the some of the artist play data and only use the rest to train model. Then, this held-out data can be interpreted as a collection of \"good\" recommendations for each user. The recommender is asked to rank all items in the model, and the rank of the held-out artists are examined. Ideally the recommender places all of them at or near the top of the list.\n",
    "\n",
    "The recommender's score can then be computed by comparing all held-out artists' ranks to the rest.  The fraction of pairs where the held-out artist is ranked higher is its score. 1.0 is perfect, 0.0 is the worst possible score, and 0.5 is the expected value achieved from randomly ranking artists. \n",
    "\n",
    "AUC(Area Under the Curve) can be used as a metric to evaluate model. It is also viewed as the probability that a randomly-chosen \"good\" artist ranks above a randomly-chosen \"bad\" artist.\n",
    "\n",
    "Next, we split the training data into 2 parts: `trainData` and `cvData` with ratio 0.9:0.1 respectively, where `trainData` is the dataset that will be used to train model. Then we write a function to calculate AUC to evaluate the quality of our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "#### Question 9.1\n",
    "<div class=\"alert alert-info\">\n",
    "Split the data into `trainData` and `cvData` with ratio 0.9:0.1 and use the first part to train a statistic model with:\n",
    "<ul>\n",
    "<li>`rank`=10</li>\n",
    "<li>`iterations`=5</li>\n",
    "<li>`lambda_`=0.01</li>\n",
    "<li>`alpha`=1.0</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the seed to obtain a deterministic result in that notebook\n",
    "trainData, cvData = allData.randomSplit([.9, .1], seed=234)\n",
    "trainData.cache()\n",
    "cvData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "model = ALS.trainImplicit(  trainData,\n",
    "                            rank=10,\n",
    "                            iterations=5,\n",
    "                            lambda_=0.01,\n",
    "                            alpha=1.0 )\n",
    "t1 = time()\n",
    "print(\"finish training model in {:.3f} secs\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this model as well\n",
    "! hdfs dfs -rm -R -f -skipTrash lastfm_model_2.spark\n",
    "t0 = time()\n",
    "model.save( sc , \"lastfm_model_2.spark\")\n",
    "t1 = time()\n",
    "print(\"finish loading model in {:.3f} secs\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And reload it\n",
    "t0 = time()\n",
    "model = MatrixFactorizationModel.load(sc, \"lastfm_model_2.spark\")\n",
    "t1 = time()\n",
    "print(\"finish loading model in {:.3f} secs\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area under the ROC curve: a function to compute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all unique artistId, and broadcast them\n",
    "allItemIDs = np.array(allData.map(lambda x: x[1]).distinct().collect())\n",
    "bAllItemIDs = sc.broadcast(allItemIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "# Depend on the number of item in userIDAndPosItemIDs,\n",
    "# create a set of \"negative\" products for each user. These are randomly chosen\n",
    "# from among all of the other items, excluding those that are \"positive\" for the user.\n",
    "# NOTE 1: mapPartitions operates on many (user,positive-items) pairs at once\n",
    "# NOTE 2: flatMap breaks the collections above down into one big set of tuples\n",
    "def xtractNegative(userIDAndPosItemIDs):\n",
    "    def pickEnoughNegatives(line):\n",
    "        userID = line[0]\n",
    "        posItemIDSet = set(line[1])\n",
    "        #posItemIDSet = line[1]\n",
    "        negative = []\n",
    "        allItemIDs = bAllItemIDs.value\n",
    "        # Keep about as many negative examples per user as positive. Duplicates are OK.\n",
    "        i = 0\n",
    "        while (i < len(allItemIDs) and len(negative) < len(posItemIDSet)):\n",
    "            itemID = allItemIDs[randint(0,len(allItemIDs)-1)]\n",
    "            if itemID not in posItemIDSet:\n",
    "                negative.append(itemID)\n",
    "            i += 1\n",
    "        \n",
    "        # Result is a collection of (user,negative-item) tuples\n",
    "        return map(lambda itemID: (userID, itemID), negative)\n",
    "\n",
    "    # Init an RNG and the item IDs set once for partition\n",
    "    # allItemIDs = bAllItemIDs.value\n",
    "    return map(pickEnoughNegatives, userIDAndPosItemIDs)\n",
    "\n",
    "def ratioOfCorrectRanks(positiveRatings, negativeRatings):\n",
    "    \n",
    "    # find number elements in arr that has index >= start and has value smaller than x\n",
    "    # arr is a sorted array\n",
    "    def findNumElementsSmallerThan(arr, x, start=0):\n",
    "        left = start\n",
    "        right = len(arr) -1\n",
    "        # if x is bigger than the biggest element in arr\n",
    "        if start > right or x > arr[right]:\n",
    "            return right + 1\n",
    "        mid = -1\n",
    "        while left <= right:\n",
    "            mid = (left + right) // 2\n",
    "            if arr[mid] < x:\n",
    "                left = mid + 1\n",
    "            elif arr[mid] > x:\n",
    "                right = mid - 1\n",
    "            else:\n",
    "                while mid-1 >= start and arr[mid-1] == x:\n",
    "                    mid -= 1\n",
    "                return mid\n",
    "        return mid if arr[mid] > x else mid + 1\n",
    "    \n",
    "    ## AUC may be viewed as the probability that a random positive item scores\n",
    "    ## higher than a random negative one. Here the proportion of all positive-negative\n",
    "    ## pairs that are correctly ranked is computed. The result is equal to the AUC metric.\n",
    "    correct = 0 ## L\n",
    "    total = 0 ## L\n",
    "    \n",
    "    # sorting positiveRatings array needs more cost\n",
    "    #positiveRatings = np.array(map(lambda x: x.rating, positiveRatings))\n",
    "\n",
    "    negativeRatings = list(map(lambda x:x.rating, negativeRatings))\n",
    "    \n",
    "    #np.sort(positiveRatings)\n",
    "    negativeRatings.sort()# = np.sort(negativeRatings)\n",
    "    total = len(positiveRatings)*len(negativeRatings)\n",
    "    \n",
    "    for positive in positiveRatings:\n",
    "        # Count the correctly-ranked pairs\n",
    "        correct += findNumElementsSmallerThan(negativeRatings, \n",
    "                                              positive.rating)\n",
    "        \n",
    "    ## Return AUC: fraction of pairs ranked correctly\n",
    "    return float(correct) / total\n",
    "\n",
    "def calculateAUC(positiveData, bAllItemIDs, predictFunction):\n",
    "    # Take held-out data as the \"positive\", and map to tuples\n",
    "    positiveUserProducts = positiveData.map(lambda r: (r[0], r[1]))\n",
    "    # Make predictions for each of them, including a numeric score, and gather by user\n",
    "    positivePredictions = predictFunction(positiveUserProducts)\\\n",
    "                            .groupBy(lambda r: r.user)\n",
    "    \n",
    "    # Create a set of \"negative\" products for each user. These are randomly chosen \n",
    "    # from among all of the other items, excluding those that are \"positive\" for the user. \n",
    "    negativeUserProducts = (positiveUserProducts\n",
    "                                .groupByKey()\n",
    "                                .mapPartitions(xtractNegative)\n",
    "                                .flatMap(lambda x: x))\n",
    "    # Make predictions on the rest\n",
    "    negativePredictions = predictFunction(negativeUserProducts).groupBy(lambda r: r.user)\n",
    "    \n",
    "    return (\n",
    "            positivePredictions.join(negativePredictions)\n",
    "                .values()\n",
    "                .map(\n",
    "                    lambda positive_negativeRatings: \n",
    "                        ratioOfCorrectRanks(\n",
    "                            positive_negativeRatings[0], \n",
    "                            positive_negativeRatings[1]\n",
    "                        )\n",
    "                )\n",
    "                .mean()\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Question 9.2\n",
    "<div class=\"alert alert-info\">\n",
    "Using part `cvData` and function `calculateAUC` to compute the AUC of the trained model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "auc = calculateAUC( cvData , bAllItemIDs, model.predictAll)\n",
    "t1 = time()\n",
    "print(\"auc=\",auc)\n",
    "print(\"finish in {:.3f} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "We observe when the ratio of training data is high, the AUC on the validation data is generaly better. However, it increases the risk of overfitting if there is too few validation data and/or the dataset is not heteregeneous.\n",
    "<br>\n",
    "Furthermore, the computing time is smaller when dealing with a smaller number of training data. A good compromise could be be at 80/20 % or maybe even 90/10%.\n",
    "</p>\n",
    "<p>\n",
    "As said, a cross validation approach would also be better but way more computationnaly expensive.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9.3\n",
    "<div class=\"alert alert-info\">\n",
    "Now we have the AUC of our model, it’s helpful to benchmark this against a simpler approach. For example, consider recommending the globally most-played artists to every user. This is not personalized, but is simple and may be effective.   \n",
    "<ul></ul>\n",
    "Implement this simple pupolarity-based prediction algorithm, evaluate its AUC score, and compare to the results achieved by the more sophisticated ALS algorithm.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bListenCount = sc.broadcast(trainData\n",
    "                                .map(lambda r: (r[1], r[2]))\n",
    "                                .reduceByKey(lambda a, b: a+b)\n",
    "                                .collectAsMap())\n",
    "\n",
    "def predictMostListened(allData):\n",
    "    return allData.map(\n",
    "        lambda r: Rating(r[0], \n",
    "                         r[1], \n",
    "                         bListenCount.value.get( r[1] , 0.0))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "auc = calculateAUC(cvData , bAllItemIDs, predictMostListened)\n",
    "t1 = time()\n",
    "print(\"auc=\",auc)\n",
    "print(\"finish in {:.3f} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Personalized recommendations with ALS\n",
    "\n",
    "In the previous section, we build our models with some given paramters without any knowledge about them. Actually, choosing the best parameters' values is very important. It can significantly affect the quality of models. Especially, with the current implementation of ALS in MLLIB, these parameters are not learned by the algorithm, and must be chosen by the caller. The following parameters should get consideration before training models:\n",
    "\n",
    "* `rank = 10`: the number of latent factors in the model, or equivalently, the number of columns $k$ in the user-feature and product-feature matrices. In non-trivial cases, this is also their rank. \n",
    "\n",
    "* `iterations = 5`: the number of iterations that the factorization runs. Instead of runing the algorithm until RMSE converged which actually takes very long time to finish with large datasets, we only let it run in a given number of iterations. More iterations take more time but may produce a better factorization.\n",
    "\n",
    "* `lambda_ = 0.01`: a standard overfitting parameter. Higher values resist overfitting, but values that are too high hurt the factorization's accuracy.\n",
    "\n",
    "*  `alpha = 1.0`: controls the relative weight of observed versus unobserved userproduct interactions in the factorization. \n",
    "\n",
    "Although all of them have impact on the models' quality, `iterations` is more of a constraint on resources used in the factorization. So, `rank`, `lambda_` and `alpha` can be considered hyperparameters to the model. \n",
    "We will try to find \"good\" values for them. Indeed, the values of hyperparameter are not necessarily optimal. Choosing good hyperparameter values is a common problem in machine learning. The most basic way to choose values is to simply try combinations of values and evaluate a metric for each of them, and choose the combination that produces the best value of the metric. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "#### Question 10.1\n",
    "<div class=\"alert alert-info\">\n",
    "For simplicity, assume that we want to explore the following parameter space: $ rank \\in \\{10, 50\\}$, $lambda\\_ \\in \\{1.0, 0.0001\\}$ and $alpha \\in \\{1.0, 40.0\\}$.\n",
    "\n",
    "Find the best combination of them in terms of the highest AUC value.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for rank in [10, 50]:\n",
    "    for lambda_ in [1.0, 0.0001]:\n",
    "        for alpha in [1.0, 40.0]:\n",
    "            print(\"Train model with rank={} lambda_={} alpha={}\"\n",
    "                      .format(rank, lambda_, alpha))\n",
    "            # with each combination of params, we should run multiple times and get avg\n",
    "            # for simple, we only run one time.\n",
    "            model = ALS.trainImplicit(trainData,\n",
    "                                        rank=rank,\n",
    "                                        iterations=5,\n",
    "                                        lambda_=lambda_,\n",
    "                                        alpha=alpha)\n",
    "            \n",
    "            auc = calculateAUC( cvData , bAllItemIDs, model.predictAll )\n",
    "            \n",
    "            evaluations.append(((rank, lambda_, alpha), auc))\n",
    "            \n",
    "            unpersist(model)\n",
    "            \n",
    "t1 = time()\n",
    "print(\"Nested loops finished in {:.3f} seconds\".format(t1 - t0))\n",
    "\n",
    "evaluations.sort( key=lambda x: x[1], reverse=True )\n",
    "                 \n",
    "evalDataFrame = pd.DataFrame(data=evaluations, \n",
    "                             columns=[\"params\", \"score\"])\n",
    "\n",
    "trainData.unpersist()\n",
    "cvData.unpersist()\n",
    "evalDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "We here use the training data to train the model and compute the auc score.\n",
    "<br>\n",
    "The results here are very specific to the portion trained and the validation data. However, it would be computationnaly much more costly to use the k-fold cross-validation method (it would require k times the current running time, which is already high). \n",
    "<br>\n",
    "Furthermore, this approach of nested-loops evaluation of the parameters is not the most efficient for very precise systems where parameters have to be very precise and therefore the number of tested parameters is higher and the time complexity is then in <code>O(exp(param_number))</code>.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10.2 \n",
    "<div class=\"alert alert-info\">\n",
    "Using \"optimal\" hyper-parameters in question 10.1, re-train the model and show top-5 artist names recommendated for user `2093760`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestTraining = evalDataFrame[\"params\"]\\\n",
    "                    .iloc[evalDataFrame[\"score\"].idxmax()]\n",
    "\n",
    "params = {\"rank\":bestTraining[0], \n",
    "          \"iterations\":5, \n",
    "          \"lambda_\":bestTraining[1], \n",
    "          \"alpha\":bestTraining[2]}\n",
    "\n",
    "print(\"Best parameters: rank={rank} lambda_={lambda_} alpha={alpha}\"\n",
    "          .format(**params))\n",
    "\n",
    "model = ALS.trainImplicit( trainData, **params )\n",
    "allData.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userID = 2093760\n",
    "recommendations = model.recommendProducts(userID, 6)\n",
    "\n",
    "recommendedProductIDs = { r.product for r in recommendations }\n",
    "\n",
    "print_recommendations(userID, recommendedProductIDs)\n",
    "\n",
    "unpersist(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "Note: The unknown artist exists in this dataset as the artistID 1034635. As a matter of fact it is a generic artist that is used in replacement for the songs of which the artist are not filled. So we can guess that there are a huge number of these (because nothing is perfect in real data!) and the number of listenings makes that this is recommended to other users whereas this is very generic and does not fit the user taste. Therefore this artist should be removed from the dataset. (For the sake of simplicity we added a 6th artist to be predicted instead of removing the unknown artist and retrain the whole model.)\n",
    "</p>\n",
    "<br>\n",
    "<p class=\"alert alert-warning\">\n",
    "The prediction seems not to be deterministic. This is due to the random values that are used to initialize the feature matrix. Therefore we onlu reach a local minimum and the algorithm is not deterministic. However it still provides good results in general.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Additionnal operations\n",
    "\n",
    "In this part, we are doing some extra manipulations to improve the recommendation system or just to compare the techique we used to other existing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 A more efficient data cleaning method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11.1 Removing alias chains**\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "The way we merged the aliases for mispelled artist IDs is not the most efficient one.\n",
    "</p>\n",
    "<p>\n",
    "The following implementation is an experiment on how we could improve the time complexity of the operation that eliminates the alias chains.\n",
    "</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimporting aliases\n",
    "\n",
    "rawArtistAlias = sc.textFile(base + \"artist_alias.txt\")\n",
    "\n",
    "artistAlias = (\n",
    "                rawArtistAlias\n",
    "                    # extract fields using function xtractFields\n",
    "                    .map( xtractFields )\n",
    "    \n",
    "                    # filter out the special tuples\n",
    "                    .filter( lambda x:x!=(-1,-1) )\n",
    "    \n",
    "                    # collect result to the driver as a \"dictionary\"\n",
    "                    .collectAsMap()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditionnal method\n",
    "\n",
    "artistAlias_m1 = artistAlias.copy()\n",
    "loops_number =20\n",
    "\n",
    "t0 = time()\n",
    "for _ in range(loops_number):\n",
    "    for k, v in artistAlias_m1.items():\n",
    "        artistAlias_m1[k] = artistAlias_m1.get(v, v)\n",
    "t1 = time()\n",
    "print(\"This method takes {:.3f} seconds.\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an accumulation point\n",
    "artistAlias_m2 = artistAlias.copy()\n",
    "\n",
    "def accumulation_points(id_):\n",
    "    \"\"\"\n",
    "        This recursive function changes the \n",
    "        dict values to the accumulation points \n",
    "        for each key in the alias chain.\n",
    "    \"\"\"\n",
    "    if (id_ in artistAlias_m2.keys() \n",
    "        and\n",
    "        artistAlias_m2[id_]!=id_):\n",
    "        \n",
    "        buffer = accumulation_points(artistAlias_m2[id_])\n",
    "        artistAlias_m2[id_] = buffer\n",
    "        return buffer\n",
    "    else:\n",
    "        return id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "for k in artistAlias_m2.keys():\n",
    "    accumulation_points(k)\n",
    "t1 = time()\n",
    "\n",
    "print(\"This method takes {:.3f} seconds.\".format(t1-t0))\n",
    "print(\"The results are identicals!\" if artistAlias_m1 == artistAlias_m2 else \"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "This recursive method is very efficient compared to the one that is used is used before. It takes advantage of the `O(1)` access and modification complexity of a dictionnary and is in `O(nlog(n))` for the mean time complexity and in `O(n^2)` for the worse case which is a dictionnary-wide chain.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a map implementation !\n",
    "\n",
    "artistAlias_m3 = artistAlias.copy()\n",
    "\n",
    "t0 = time()\n",
    "# Iterate over the map object (which is an iterable)\n",
    "for _ in map(accumulation_points, artistAlias_m3.keys()):\n",
    "    pass\n",
    "t1 = time()\n",
    "\n",
    "print(\"This method takes {:.3f} seconds.\".format(t1-t0))\n",
    "print(\"The results are identicals!\" if artistAlias_m1 == artistAlias_m3 else \"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "The result is even faster with a map implementation (which is still not distributed but the runtime is very small)! \n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<div class=\"alert alert-success\">\n",
    "<p>\n",
    "Another method would have been to draw the DAG designed by the original dictionnary (vertices = artistIDs, oriented edges = alias relation) and to get the attractor for each point. Unfortunately the GraphX API is not available for PySpark yet (and the current hack to make it work is not working here: <code>pyspark --packages graphframes:graphframes:0.1.0-spark1.6</code>).\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeing memory\n",
    "artistAlias = artistAlias_m2\n",
    "del artistAlias_m1, artistAlias_m2, artistAlias_m3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11.2 Removing outliers**\n",
    "\n",
    "Cf question 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanUserArtistDF = (\n",
    "    newUserArtistDF\n",
    "        .alias(\"df\")\n",
    "    \n",
    "        # First step for removing the outlier artists:\n",
    "        # Computing their the sum of their playcounts\n",
    "        .join(newUserArtistDF.groupBy( \"artistID\" )\n",
    "                             .sum( \"playCount\" )\n",
    "                             .alias(\"sumArtistDF\"),\n",
    "              col(\"sumArtistDF.artistID\") == col(\"df.artistID\") , \n",
    "              \"left_outer\")\n",
    "    \n",
    "        # Keeping artists in range 40%-100% in play count cumulative chart\n",
    "        .filter(col(\"sumArtistDF.sum(playCount)\") >= int(artist_range[\"min\"]))\n",
    "    \n",
    "        #Same thing with the users\n",
    "        .join(newUserArtistDF.groupBy( \"userID\" )\n",
    "                             .sum( \"playCount\" )\n",
    "                             .alias(\"sumUserDF\"),\n",
    "              col(\"sumUserDF.userID\") == col(\"df.userID\") , \n",
    "              \"left_outer\")\n",
    "    \n",
    "        # Keeping users in range 10%-90%\n",
    "        .filter(col(\"sumUserDF.sum(playCount)\") >= int(user_range[\"min\"]))\n",
    "        .filter(col(\"sumUserDF.sum(playCount)\") <= int(user_range[\"max\"]))\n",
    "    \n",
    "        # Keeping in memory only the legacy structure\n",
    "        .select(col(\"df.userID\"), col(\"df.artistID\"), col(\"df.playCount\"))\n",
    "    \n",
    "        # Caching the dataframe to use it commonly\n",
    "        .cache()\n",
    ")\n",
    "\n",
    "cleanUserArtistDF.rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the number of unique artists once removed the outliers\n",
    "\n",
    "uniqueArtists = cleanUserArtistDF.select(\"artistID\").distinct().count()\n",
    "\n",
    "print(\"Total n. of artists: \", uniqueArtists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Clustering users before predicting with K-medoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "<br>\n",
    "<div>\n",
    "<p>When the user-behavior data is sparse, similar users are difficult to be found in collaborative filtering. But similar\n",
    "users are a critical factor for collaborative filtering. There's a way to ally content-based method and collaborative filtering by clustering users using K-medoids clustering algorithm.</p>\n",
    "<p> \n",
    "    \n",
    " <li> First, we analyze the content of resources by extracting resources features. And we utilize the resources features to cluster similar resources.</li>\n",
    "\n",
    " <li>Second, we apply the condensed user-behavior matrix to generate recommendation resources in user-based collaborative filtering.</li>\n",
    "\n",
    "</p>\n",
    "<p>K-medoids is a clustering algorithm that is related to the k-means algorithm. The k-medoids is a partitioning algorithm that divides the data set up into separate clusters. The algorithm attempts to minimize the squared error, which is the distance between points in the cluster and a point that is designated as the center (medoid) of the cluster. A medoid is considered as an object of a cluster whose average dissimilarity to all the\n",
    "objects in a cluster is minimal.</p>\n",
    "<p>K-medoids algorithm is comparatively robust than K-means particularly in the context of outliers and noise because\n",
    "the K-means algorithm is sensetive to outliers. Because a mean is sensetive to the outliers, in our case for example, if you look at a playcount of a user in a whole group of users listening to Taylor Swift (for example) and you add another playcount which is very high the average playcount will shift a lot. So instead of taking the mean value of object in a cluster as a centroid the most centrally located object is used in the cluster as a medoid in K-medoids. The user gives the number of clusters required. This algorithm depends on the minimizing the sum of dissimilarities (distance) between each object and its medoid.</p>\n",
    "<p>However K-medoids has some limitations. K-medoids clustering algorithm centroids are initially selected by the user. Therefore, performance of these algorithms depends on this manual selection of centroids. It works\n",
    "inefficiently for large data sets due to its complexity.</p>\n",
    "<p>\n",
    "Thus for all the reasons exposed we chose this method for user clustering.\n",
    "</p>\n",
    "<br>\n",
    "The steps are as follow :\n",
    "    <li>We normalize the playcout (corresponding to each artist) per user: we won't only consider the play count and artist listened to find out similarities, we will consider 'frequencies'</li>\n",
    "    <li>We will use n-gram method and combine it to the frequencies by using a threshold</li>\n",
    "    <li>Then we compute the n-gram similarity to draw the distance matrix</li>\n",
    "    <li>We compute the Kmedoid applied to the distance matrix</li>\n",
    "\n",
    "<b> TODO ? </b> In order to improve the performance of recommendation, we expect to condense the user-behavior data so as to find more similar users. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "<br>\n",
    "<div>\n",
    "First, we have to consider that if two users listen to the same artists it won't be at the same frequency. We wanted to make sure that we won't only take into account the fact that two user can listen to the same artists but also that they like the same artists. That's why we will use frequencies and a threshold to make sure that the artists listened that we will take into account are the artists the user like.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threeshold_value = .03\n",
    "\n",
    "userSum = newUserArtistDF.groupBy(\"userID\").sum(\"playCount\")\n",
    "\n",
    "\n",
    "#userFreq = newUserArtistDF.join(userSum, \"userID\"=userID).withColumn(\"playRate\", \"PlayCount\"/\"sum(\"PlayCount\")\")\n",
    "\n",
    "userFrequencies = (\n",
    "    newUserArtistDF.alias(\"df\")\n",
    "                .join(userSum.alias(\"uSum\"), \n",
    "                      col(\"df.userID\") == col(\"uSum.userID\"), \n",
    "                      \"left_outer\")\n",
    "                .withColumn('playRate', \n",
    "                            col('playCount')/col('sum(playCount)'))\n",
    "                .select(col('df.userID'), 'artistID', 'playRate')\n",
    "                .filter(\"playRate>=\"+str(Threeshold_value))\n",
    ")\n",
    "\n",
    "userFrequencies.show(5)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Similarity\n",
    "                            \n",
    "<p>Now, given two userss and their corresponding sets S1 and S2 of n-grams, White proposes to consider the following n-gram similarity:\n",
    "$$s(S1,S2)= \\frac{2·|S1∩S2|}{|S1| + |S2|}$$\n",
    "</p>\n",
    "<p>For which we note that 0 ≤ s ≤ 1. We also observe that s = 1 if S1 and S2 are equal and that s=0 if S1 and S2 are disjoint, i.e. do not share any n-gram.</p>\n",
    "<p>In order to turn this similarity measure into a distance, we simply define\n",
    "$$d(S1, S2) = 1 − s(S1, S2)$$\n",
    "such that d=0 if S1 and S2 are equal and d>0 otherwise.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute a function to find the n-grams for a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_similarity(userID1, userID2):\n",
    "    user1 = userFrequencies.filter(col(\"userID\") == userID1).select(\"ArtistId\")\n",
    "    user2 = userFrequencies.filter(col(\"userID\") == userID2).select(\"ArtistId\")\n",
    "    len_user1 = user1.count()\n",
    "    len_user2 = user2.count()\n",
    "    intersect = user1.intersection(user2)\n",
    "    len_intersect = intersect.distinct().count()\n",
    "    dist = 1-2*len_intersect/float(len_user1+len_user2)\n",
    "    return dist\n",
    "\n",
    "def closest_medoid(user, medoids):\n",
    "    distance_from_meds = [ngrams_similarity(user, med) for med in medoids.value]\n",
    "    return (distance_from_meds.index(min(distance_from_meds)), (user, 1))\n",
    "\n",
    "def closests_medoid(users, medoids):\n",
    "    return [closest_medoid(us, medoids) for user in medoids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_kmedoid(X, k, medoids, maxiter=300):\n",
    "    X_para = sc.parallelize(X).cache()\n",
    "    users = None\n",
    "    for _ in range(maxiter):\n",
    "        bcMedoids = sc.broadcast(medoids)\n",
    "        closest = X_para.mapPartitions(lambda x: closests_medoid(x, bcMedoids))\n",
    "        new_medoids = closest.reduceByKey(lambda a, b: (a[0] + b[0], a[1]+b[1]))\\\n",
    "                            .map(lambda a: (a[1][0]/a[1][1]))\\\n",
    "                            .collect()\n",
    "        if np.equal(new_medoids, medoids).all(): break\n",
    "        else: medoids = medoids\n",
    "    return medoids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Medoid Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common realisation of k-medoid clustering is the Partitioning Around Medoids (PAM) algorithm. PAM uses a greedy search which may not find the optimum solution, but it is faster than exhaustive search. It works as follows:\n",
    "\n",
    "1. Initialize: randomly initialize an array of k medoid indices\n",
    "2. Initialize a dictionary to represent clusters\n",
    "3. For k in range kmax:\n",
    "    1. Determine clusters, i.e. arrays of data indices:\n",
    "    2. Update cluster medoids\n",
    "    3. Check for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def kMedoids(D, k, tmax=100):\n",
    "    # determine dimensions of distance matrix D\n",
    "    m, n = D.shape\n",
    "\n",
    "    # randomly initialize an array of k medoid indices\n",
    "    M = np.sort(np.random.choice(n, k))\n",
    "\n",
    "    # create a copy of the array of medoid indices\n",
    "    Mnew = np.copy(M)\n",
    "\n",
    "    # initialize a dictionary to represent clusters\n",
    "    C = {}\n",
    "    for t in range(tmax):\n",
    "        # determine clusters, i.e. arrays of data indices\n",
    "        J = np.argmin(D[:,M], axis=1)\n",
    "        for kappa in range(k):\n",
    "            C[kappa] = np.where(J==kappa)[0]\n",
    "\n",
    "        # update cluster medoids\n",
    "        for kappa in range(k):\n",
    "            J = np.mean(D[np.ix_(C[kappa],C[kappa])],axis=1)\n",
    "            j = np.argmin(J)\n",
    "            Mnew[kappa] = C[kappa][j]\n",
    "        np.sort(Mnew)\n",
    "\n",
    "        # check for convergence\n",
    "        if np.array_equal(M, Mnew) :\n",
    "            break\n",
    "\n",
    "        M = np.copy(Mnew)\n",
    "    else:\n",
    "        J = np.argmin(D[:,M], axis=1)\n",
    "        for  kappa in range(k):\n",
    "            C[kappa] = np.where(J==kappa)[0]\n",
    "\n",
    "    # return results\n",
    "    return M, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know the number of cluster that best suits the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Prediction\n",
    "\n",
    "<p>LightFM is a Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback.</p>\n",
    "\n",
    "<p>It also makes it possible to incorporate both item and user metadata into the traditional matrix factorization algorithms. It represents each user and item as the sum of the latent representations of their features, thus allowing recommendations to generalise to new items (via item features) and to new users (via user features).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "The structure of the LightFM model is motivated by two considerations.\n",
    "1. The model must be able to learn user and item representations from interaction data: if items described as ‘ball gown and ‘pencil skirt’ are consistently all liked by users, the model must learn that ball gowns are similar to pencil skirts.\n",
    "2. The model must be able to compute recommendations for new items and users. \n",
    "\n",
    "<p>The first requirement is fulfiled by using the latent representation approach. If ball gowns and pencil skirts are both liked by the same users, their embeddings will be close together; if ball gowns and biker jackets are never liked by the same users, their embeddings will be far apart.</p>\n",
    "<p>Such representations allow transfer learning to occur. If the representations for ball gowns and pencil skirts are similar, we can confidently recommend ball gowns to a new user who has so far only interacted with pencil skirts. </p>\n",
    "<p>This is over and above what pure CB models using dimensionality reduction techniques (such as latent semantic indexing, LSI) can achieve, as these only encode information given by feature co-occurrence rather than user actions. For example, suppose that all users who look at items described as aviators also look at items described as wayfarers, but the two features never describe the same item. In this case, the LSI vector for wayfarers will not be similar to the one for aviators even though collaborative information suggests it should be.</p>\n",
    "<p>The second requirement is fulfilled by representing items and users as linear combinations of their content features. Because content features are known the moment a user or item enters the system, this allows recommendations to be made straight away. The resulting structure is also easy to understand. The representation for denim jacket is simply a sum of the representation of denim and the representation of jacket; the representation for a female user from the US is a sum of the representations of US and female users.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lightfm\n",
    "from lightfm import LightFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Model fitting is very straightforward using the main LightFM class.</p>\n",
    "\n",
    "<p>First we create a model instance with the desired latent dimensionality</p>\n",
    "\n",
    "<p>Four loss functions are available:</p>\n",
    "\n",
    "1. logistic: useful when both positive (1) and negative (-1) interactions are present.\n",
    "2. BPR: Bayesian Personalised Ranking pairwise loss. Maximises the prediction difference between a positive example and a randomly chosen negative example. Useful when only positive interactions are present and optimising ROC AUC is desired.\n",
    "3. WARP: Weighted Approximate-Rank Pairwise loss. Maximises the rank of positive examples by repeatedly sampling negative examples until rank violating one is found. Useful when only positive interactions are present and optimising the top of the recommendation list (precision@k) is desired.\n",
    "4. k-OS WARP: k-th order statistic loss. A modification of WARP that uses the k-th positive example for any given user as a basis for pairwise updates.\n",
    "\n",
    "<p>We chose BPR as it was the only one corresponding to the data we have. Indeed, it is said that it is useful when there are only positive interactions.</p>\n",
    "\n",
    "<p> [BPR] (http://blog.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/) is a method for Leanring to Rank comparable to ALS or conventional Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightFM(learning_schedule='adagrad', learning_rate=0.05, loss='bpr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User and item features can be incorporated into training by passing them into the fit method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_user_id, max_artist_id = (userArtistDF\n",
    "#        .select(max(\"userID\"),max(\"artistID\"))\n",
    "#        .collect()[0])\n",
    "\n",
    "#train = trainData.collect()\n",
    "#training_matrix=np.zeros(max_user_id, max_artist_id)\n",
    "\n",
    "# Fill the matrix\n",
    "#for rating in train:\n",
    "#    training_matrix[rating.user, rating.product] = rating.rate\n",
    "    \n",
    "# Mean of the rate for each user\n",
    "#for i in range(max_user_id):\n",
    "#    s = sum(training_matrix[i, :])\n",
    "#    if s!=0:\n",
    "#        training_matrix[i, :] = training_matrix[i, :]/float(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, epochs=20, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = model.predict(test_user_ids=, test_item_ids=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation uses asynchronous stochastic gradient descent for training. This can lead to lower accuracy when the interaction matrix (or the feature matrices) are very dense and a large number of threads is used. In practice, however, training on a sparse dataset (with 20 threads does not lead to a measurable loss of accuracy).\n",
    "<div class=\"alert alert-success\">\n",
    "<b>\n",
    "    TODO :\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_precision = precision_at_k(model, train, k=10).mean()\n",
    "test_precision = precision_at_k(model, test, k=10).mean()\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, we introduce an algorithm to do matrix factorization and the way of using it to make recommendation. Further more, we studied how to build a large-scale recommender system on SPARK using ALS algorithm and evaluate its quality. Finally, a simple approach to choose good parameters is mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- The example in section 2 is taken from [Recommender system](infolab.stanford.edu/~ullman/mmds/ch9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>\n",
    "TODO : Bibliography\n",
    "</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
